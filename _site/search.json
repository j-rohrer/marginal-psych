[
  {
    "objectID": "examples/relationship.html",
    "href": "examples/relationship.html",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "Before we start, let’s execute a helper script that loads the necessary dependencies.\n\nsource(here::here(\"scripts/load.R\"))\n\n\n\nThis document contains example 2, in which we analyze the association between relationship status and the importance people assign to friends. Data can be retrieved from the OSF but is also included in the downloadable replication package.\nIt’s a common complaint that people who enter a relationship start to neglect their friends. Here, we are going to use this to motivate an associational research question: Do people in romantic relationships, on average, assign less importance to their friends? To address this question, we analyze data that were collected in the context of a diary study on satisfaction with various aspects of life (Rohrer et al., 2024). In this study, 482 people reported whether they were in a romantic relationship of any kind (partner) and also the extent to which they considered their friendships important (friendship_importance) on a scale from 1 (not important at all) to 5 (very important).\n\n\n\n\n# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\n# Look at the variables\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\n# Rename some variables\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)\n\n\n\n\nThis code produces Figure 4: Predicted importance of friends by age in three different simple linear models that only include age as a predictor, including (1) age as a linear predictor or (2) age as a categorical predictor, or (3) age splines (B-splines with four degrees of freedom).\nRespondents’ age varies from 18 to 59 years. How do we best include this variable in our analysis? If we simply include it as a linear predictor, we assume that friendship_importance changes with age in a linear manner. If, instead, we include it as a categorical predictor (treating each year of age as its own category), we do not impose any assumptions about the functional form—but for some years, we only have few observations, resulting in a trajectory that jumps around a lot, with wide confidence intervals.\nSo, we may prefer a solution that lies somewhere between these two options. Contenders may be using coarser age categories or using polynomials. A third alternative is splines. These result in flexible, locally smooth trajectories. Unlike polynomials, splines enforce no global functional forms; unlike age categories, splines do not result in abrupt jumps in the trajectory.\n\n# Linear association\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\n# Each year of age as its own category\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n# Smoothed with splines\nage_smooth &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\n\n# Extract predictions from each of the three models\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\npred_smooth &lt;- avg_predictions(age_smooth, by = \"age\")\n\n# The following code generates a grid that expands the categorical predictions\n# so that we can plot them as a step function including a step ribbon\n# (This is just to get the visuals of Figure 4 right and nothing substantive)\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# Color scheme for Figure 4\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#D55E00\"\ncol_smooth &lt;- \"#CC79A7\"\n\n# Generate the plot\nggplot() +\n  # categorical\n  geom_line(data = pred_cat_expanded, aes(x = age, y = estimate, group = floor(age)), color = col_cat) +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  # smoothed\n  geom_line(data = pred_smooth, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_smooth, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_smooth) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Importance of friends (95% CI)\")\n\n\n\n# ggsave(here(\"plots/age.png\"), width = 4, height = 3)\n\n\n\n\n\nmod &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\n\n# Predictions for a 20 year old single\npredictions(mod, newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  4.66\n                  0.243\n                  19.2\n                  &lt;0.001\n                  269.4\n                  4.18\n                  5.13\n                \n        \n      \n    \n\n\n# Slope of age for a 20 year old single\nslopes(mod, variables = \"age\", newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  0.00329\n                  0.104\n                  0.0318\n                  0.975\n                  0.0\n                  -0.2\n                  0.207\n                \n        \n      \n    \n\n\n# Predictions for everybody in the data\npredictions(mod)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  4.46\n                  0.1485\n                  30.0\n                  &lt;0.001\n                  654.5\n                  4.17\n                  4.75\n                \n                \n                  4.58\n                  0.1362\n                  33.6\n                  &lt;0.001\n                  819.9\n                  4.31\n                  4.85\n                \n                \n                  4.45\n                  0.1131\n                  39.3\n                  &lt;0.001\n                  Inf\n                  4.23\n                  4.67\n                \n                \n                  4.41\n                  0.0994\n                  44.4\n                  &lt;0.001\n                  Inf\n                  4.22\n                  4.61\n                \n                \n                  4.52\n                  0.1447\n                  31.2\n                  &lt;0.001\n                  709.7\n                  4.24\n                  4.81\n                \n                \n                  4.66\n                  0.1341\n                  34.8\n                  &lt;0.001\n                  876.7\n                  4.40\n                  4.92\n                \n                \n                  4.63\n                  0.1872\n                  24.7\n                  &lt;0.001\n                  446.4\n                  4.26\n                  5.00\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.67\n                  0.1104\n                  42.2\n                  &lt;0.001\n                  Inf\n                  4.45\n                  4.88\n                \n        \n      \n    \n\n\n# Main result: Difference in friendship importance with and without partner, holding constant age and gender\navg_comparisons(mod, variables = \"partner\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.072\n                  0.0804\n                  -0.896\n                  0.37\n                  1.4\n                  -0.23\n                  0.0856\n                \n        \n      \n    \n\n\n# Generate separate estimates of the difference, by gender\navg_comparisons(mod, variables = \"partner\", by = \"gender\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  female\n                  -0.0738\n                  0.0946\n                  -0.780\n                  0.436\n                  1.2\n                  -0.259\n                  0.112\n                \n                \n                  male\n                  -0.0675\n                  0.1529\n                  -0.442\n                  0.659\n                  0.6\n                  -0.367\n                  0.232\n                \n        \n      \n    \n\n\n# Compare these gender-specific estimates\navg_comparisons(mod, variables = \"partner\", by = \"gender\", hypothesis = \"b2 - b1 = 0\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b2-b1=0\n                  0.00625\n                  0.18\n                  0.0348\n                  0.972\n                  0.0\n                  -0.346\n                  0.359\n                \n        \n      \n    \n\n\n# For the sake of completeness, we may also generate age-specific estimates\n\n# Note that the wiggliness of this line will depend on how we modeled age\ncomp &lt;- avg_comparisons(mod, variables = \"partner\", by = \"age\")\n\nggplot(comp, aes(x = age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() +\n  geom_ribbon(alpha = .1) +\n  geom_hline(yintercept = 0) +\n  ylab(\"Association partner and friendship importance\")\n\n\n\n\n\n\n\nSo far, we have simply conducted linear regressions, but that may be considered suspect given the nature of the outcome: it’s just a five-point response scale ranging from not important at all to very important. And, in fact, barely anybody used the lower response options—more than 40% picked the highest response option. This results in a distribution for which the assumptions of linear regression may be considered unrealistic.\nSo, let’s run an ordinal regression to see whether conclusions change.\nHere, we are going to fit a cumulative ordinal model with a probit link using the brms package [(Bürkner, 2018)](https://cran.r-project.org/web/packages/brms/index.html}. In essence, this approach assumes a continuous, normally distributed standardized latent variable (“true” friendship_importance) which is translated into the ordinal response variable following thresholds that are estimated from the data; for example, people who score -3 or less on the standardized latent variable may report that their friends are not important at all, people who score more than that but below -2.7 may pick the second lowest response option, and so on (see Bürkner & Vuorre, 2019 for a proper introduction to these models).\nThe rest of the model specification remains unchanged:\nWe can evaluate the association of interest using the same average comparison as before:\n\navg_comparisons(mod_ord, variables = \"partner\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00132\n                  -0.00798\n                  0.0115\n                \n                \n                  2\n                  0.00338\n                  -0.00757\n                  0.0156\n                \n                \n                  3\n                  0.02108\n                  -0.01542\n                  0.0597\n                \n                \n                  4\n                  0.02925\n                  -0.00152\n                  0.0613\n                \n                \n                  5\n                  -0.05541\n                  -0.13218\n                  0.0181\n                \n        \n      \n    \n\n\n\nThe resulting output, however, differs. By default, the output shows how the probability of any response category of friendship_importance when partner changes from 0 to 1. The response categories 1 to 4 become slightly more likely with partner = 1, whereas the probability of giving the highest rating, 5, decreases by 5.5 percentage points.\nWe can also compute the comparison on the assumed underlying latent variable, which is the scale on which the model coefficients are reported in the regression output:\n\navg_comparisons(mod_ord, variables = \"partner\", type = \"link\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.148\n                  -0.364\n                  0.057\n                \n        \n      \n    \n\n\n\nAnd, as promised in the main text, we can combine these category-specific estimates on the averaged scale.\n\n# Take the response probabilities and attach the integers 1:5 to the consecutive\n# response categories to arrive at the same metric as the linear model\navg_comparisons(mod_ord, variables = \"partner\", hypothesis = ~ I(sum(x * 1:5)))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.0883\n                  -0.244\n                  0.062\n                \n        \n      \n    \n\n\n\nWhile this ensures that we ask the ordinal model precisely the same answer that we asked the linear model, it may appear a bit inconsistent given that we usually use ordinal models precisely because we don’t want to assign integer values to the response categories – although note that here we do it only in the very last step, so the model itself does not assume that the distances between adjacent response categories are the same.\n\n\n\n# Outcome as factor variable which is what clm() expects\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\n\n# Fit the model\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\n\n# Evaluate central comparison in terms of change in response probabilities\navg_comparisons(mod_ord, variables = \"partner\") \n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00170\n                  0.00436\n                  0.390\n                  0.6963\n                  0.5\n                  -0.00685\n                  0.0103\n                \n                \n                  2\n                  0.00353\n                  0.00555\n                  0.636\n                  0.5247\n                  0.9\n                  -0.00735\n                  0.0144\n                \n                \n                  3\n                  0.02129\n                  0.01963\n                  1.085\n                  0.2780\n                  1.8\n                  -0.01718\n                  0.0598\n                \n                \n                  4\n                  0.02896\n                  0.01641\n                  1.765\n                  0.0776\n                  3.7\n                  -0.00320\n                  0.0611\n                \n                \n                  5\n                  -0.05549\n                  0.03847\n                  -1.442\n                  0.1492\n                  2.7\n                  -0.13089\n                  0.0199\n                \n        \n      \n    \n\n\n# Evaluate effect on the underlying latent variable\navg_comparisons(mod_ord, variables = \"partner\", type = \"linear.predictor\") \n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  2\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  3\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  4\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  5\n                  0.147\n                  0.107\n                  1.37\n                  0.170\n                  2.6\n                  -0.0627\n                  0.356\n                \n        \n      \n    \n\n\n# Note that this estimate has the wrong sign\n# Likely to an upstream bug in clm()\n\n# Manually reconstruct\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\nd0 &lt;- transform(dat, partner = 0)\nd1 &lt;- transform(dat, partner = 1)\np0 &lt;- predict(mod_ord, newdata = d0, type = \"linear.predictor\")\np1 &lt;- predict(mod_ord, newdata = d1, type = \"linear.predictor\")\nmean(p1$eta1 - p0$eta1)\n\n[1] 0.1465469\n\n# If we do this manually, we also get the wrong sign\n# So something seems wrong about the predictions that clm returns\n\n\n\n\n\nLet’s vary model complexity to see what happens to our target quantity, the association between having a partner and friendship importance (holding constant age and gender).\n\n\n\n# No age in the model\nmod_age_0 &lt;- lm(friendship_importance ~ gender + partner +\n            partner:gender, data = dat)\ncomp_age_0 &lt;- avg_comparisons(mod_age_0, variables = \"partner\")\ncomp_age_0$model &lt;- \"Not included\"\n\n# Linear age in the model\nmod_age_1 &lt;- lm(friendship_importance ~ age + gender + partner +\n            age:gender + partner:gender + age:partner, data = dat)\ncomp_age_1 &lt;- avg_comparisons(mod_age_1, variables = \"partner\")\ncomp_age_1$model &lt;- \"Linear\"\n\n# Quadratic age on top\nmod_age_2 &lt;- lm(friendship_importance ~ poly(age, 2) + gender + partner +\n            poly(age, 2):gender + partner:gender + poly(age, 2):partner, data = dat)\ncomp_age_2 &lt;- avg_comparisons(mod_age_2, variables = \"partner\")\ncomp_age_2$model &lt;- \"Quadratic\"\n\n# Cubic age on top\nmod_age_3 &lt;- lm(friendship_importance ~ poly(age, 3) + gender + partner +\n            poly(age, 3):gender + partner:gender + poly(age, 3):partner, data = dat)\ncomp_age_3 &lt;- avg_comparisons(mod_age_3, variables = \"partner\")\ncomp_age_3$model &lt;- \"Cubic\"\n\n# Quartic age on tope\nmod_age_4 &lt;- lm(friendship_importance ~ poly(age, 4) + gender + partner +\n            poly(age, 4):gender + partner:gender + poly(age, 4):partner, data = dat)\ncomp_age_4 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_4$model &lt;- \"Quartic\"\n\n# Quintic age on tope\nmod_age_5 &lt;- lm(friendship_importance ~ poly(age, 5) + gender + partner +\n            poly(age, 5):gender + partner:gender + poly(age, 5):partner, data = dat)\ncomp_age_5 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_5$model &lt;- \"Quintic\"\n\n\n# Age with splines, df = 3\nmod_age_6 &lt;- lm(friendship_importance ~ bs(age, df = 3) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_6 &lt;- avg_comparisons(mod_age_6, variables = \"partner\")\ncomp_age_6$model &lt;- \"Splines, df = 3\"\n\n\n# Splines, df = 4 (same as model reported in main text)\nmod_age_7 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_7 &lt;- avg_comparisons(mod_age_7, variables = \"partner\")\ncomp_age_7$model &lt;- \"Splines, df = 4\"\n\n# Splines, df = 5\nmod_age_8 &lt;- lm(friendship_importance ~ bs(age, df = 5) + gender + partner +\n                  bs(age, df = 5):gender + partner:gender + bs(age, df = 5):partner, data = dat)\ncomp_age_8 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_8$model &lt;- \"Splines, df = 5\"\n\n# Splines, df = 6\nmod_age_9 &lt;- lm(friendship_importance ~ bs(age, df = 6) + gender + partner +\n                  bs(age, df = 6):gender + partner:gender + bs(age, df = 6):partner, data = dat)\ncomp_age_9 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_9$model &lt;- \"Splines, df = 6\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_age &lt;- rbind(data.frame(comp_age_0), data.frame(comp_age_1), \n                  data.frame(comp_age_2), data.frame(comp_age_3),\n                  data.frame(comp_age_4), data.frame(comp_age_5),\n                  data.frame(comp_age_6), data.frame(comp_age_7),\n                  data.frame(comp_age_8), data.frame(comp_age_9))\n\n# Numbering (for plotting purposes)\ncomp_age$no &lt;- 1:nrow(comp_age)\n\n# Generate the plot\nggplot(comp_age, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_age$no, labels = comp_age$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Way that age is included in model\") +\n  ylab(\"Target quantity (95% CI)\")\n\n\n\n# ggsave(here(\"plots/age_robustness.png\"), width = 4, height = 3)\n\n\n\n\nLet’s see whether the inclusion of interactions makes a difference\n\n# No interactions\nmod_interact_1 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner, data = dat)\ncomp_interact_1 &lt;- avg_comparisons(mod_interact_1, variables = \"partner\")\ncomp_interact_1$model &lt;- \"No interaction\"\n\n# Add two-way interactions (same as model reported in main text)\nmod_interact_2 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                       bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\ncomp_interact_2 &lt;- avg_comparisons(mod_interact_2, variables = \"partner\")\ncomp_interact_2$model &lt;- \"Two-way interactions\"\n\n# Add three-way interaction\nmod_interact_3 &lt;- lm(friendship_importance ~ bs(age, df = 4)*gender*partner, data = dat)\ncomp_interact_3 &lt;- avg_comparisons(mod_interact_3, variables = \"partner\")\ncomp_interact_3$model &lt;- \"Three-way interaction\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_interact &lt;- rbind(data.frame(comp_interact_1), data.frame(comp_interact_2), data.frame(comp_interact_3))\n\n# Numbering (for plotting purposes)\ncomp_interact$no &lt;- 1:nrow(comp_interact)\n\n# Plot the results\nggplot(comp_interact, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_interact$no, labels = comp_interact$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Interactions included\") +\n  ylab(\"Estimated difference in friendship importance\")"
  },
  {
    "objectID": "examples/relationship.html#overview",
    "href": "examples/relationship.html#overview",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "This document contains example 2, in which we analyze the association between relationship status and the importance people assign to friends. Data can be retrieved from the OSF but is also included in the downloadable replication package.\nIt’s a common complaint that people who enter a relationship start to neglect their friends. Here, we are going to use this to motivate an associational research question: Do people in romantic relationships, on average, assign less importance to their friends? To address this question, we analyze data that were collected in the context of a diary study on satisfaction with various aspects of life (Rohrer et al., 2024). In this study, 482 people reported whether they were in a romantic relationship of any kind (partner) and also the extent to which they considered their friendships important (friendship_importance) on a scale from 1 (not important at all) to 5 (very important)."
  },
  {
    "objectID": "examples/relationship.html#read-and-clean-the-data",
    "href": "examples/relationship.html#read-and-clean-the-data",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\n# Look at the variables\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\n# Rename some variables\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)"
  },
  {
    "objectID": "examples/relationship.html#visualize-associations-with-age",
    "href": "examples/relationship.html#visualize-associations-with-age",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "This code produces Figure 4: Predicted importance of friends by age in three different simple linear models that only include age as a predictor, including (1) age as a linear predictor or (2) age as a categorical predictor, or (3) age splines (B-splines with four degrees of freedom).\nRespondents’ age varies from 18 to 59 years. How do we best include this variable in our analysis? If we simply include it as a linear predictor, we assume that friendship_importance changes with age in a linear manner. If, instead, we include it as a categorical predictor (treating each year of age as its own category), we do not impose any assumptions about the functional form—but for some years, we only have few observations, resulting in a trajectory that jumps around a lot, with wide confidence intervals.\nSo, we may prefer a solution that lies somewhere between these two options. Contenders may be using coarser age categories or using polynomials. A third alternative is splines. These result in flexible, locally smooth trajectories. Unlike polynomials, splines enforce no global functional forms; unlike age categories, splines do not result in abrupt jumps in the trajectory.\n\n# Linear association\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\n# Each year of age as its own category\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n# Smoothed with splines\nage_smooth &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\n\n# Extract predictions from each of the three models\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\npred_smooth &lt;- avg_predictions(age_smooth, by = \"age\")\n\n# The following code generates a grid that expands the categorical predictions\n# so that we can plot them as a step function including a step ribbon\n# (This is just to get the visuals of Figure 4 right and nothing substantive)\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# Color scheme for Figure 4\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#D55E00\"\ncol_smooth &lt;- \"#CC79A7\"\n\n# Generate the plot\nggplot() +\n  # categorical\n  geom_line(data = pred_cat_expanded, aes(x = age, y = estimate, group = floor(age)), color = col_cat) +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  # smoothed\n  geom_line(data = pred_smooth, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_smooth, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_smooth) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Importance of friends (95% CI)\")\n\n\n\n# ggsave(here(\"plots/age.png\"), width = 4, height = 3)"
  },
  {
    "objectID": "examples/relationship.html#model-fitting-and-interpretation-with-marginaleffects",
    "href": "examples/relationship.html#model-fitting-and-interpretation-with-marginaleffects",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "mod &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\n\n# Predictions for a 20 year old single\npredictions(mod, newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  4.66\n                  0.243\n                  19.2\n                  &lt;0.001\n                  269.4\n                  4.18\n                  5.13\n                \n        \n      \n    \n\n\n# Slope of age for a 20 year old single\nslopes(mod, variables = \"age\", newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  0.00329\n                  0.104\n                  0.0318\n                  0.975\n                  0.0\n                  -0.2\n                  0.207\n                \n        \n      \n    \n\n\n# Predictions for everybody in the data\npredictions(mod)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  4.46\n                  0.1485\n                  30.0\n                  &lt;0.001\n                  654.5\n                  4.17\n                  4.75\n                \n                \n                  4.58\n                  0.1362\n                  33.6\n                  &lt;0.001\n                  819.9\n                  4.31\n                  4.85\n                \n                \n                  4.45\n                  0.1131\n                  39.3\n                  &lt;0.001\n                  Inf\n                  4.23\n                  4.67\n                \n                \n                  4.41\n                  0.0994\n                  44.4\n                  &lt;0.001\n                  Inf\n                  4.22\n                  4.61\n                \n                \n                  4.52\n                  0.1447\n                  31.2\n                  &lt;0.001\n                  709.7\n                  4.24\n                  4.81\n                \n                \n                  4.66\n                  0.1341\n                  34.8\n                  &lt;0.001\n                  876.7\n                  4.40\n                  4.92\n                \n                \n                  4.63\n                  0.1872\n                  24.7\n                  &lt;0.001\n                  446.4\n                  4.26\n                  5.00\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.67\n                  0.1104\n                  42.2\n                  &lt;0.001\n                  Inf\n                  4.45\n                  4.88\n                \n        \n      \n    \n\n\n# Main result: Difference in friendship importance with and without partner, holding constant age and gender\navg_comparisons(mod, variables = \"partner\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.072\n                  0.0804\n                  -0.896\n                  0.37\n                  1.4\n                  -0.23\n                  0.0856\n                \n        \n      \n    \n\n\n# Generate separate estimates of the difference, by gender\navg_comparisons(mod, variables = \"partner\", by = \"gender\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  female\n                  -0.0738\n                  0.0946\n                  -0.780\n                  0.436\n                  1.2\n                  -0.259\n                  0.112\n                \n                \n                  male\n                  -0.0675\n                  0.1529\n                  -0.442\n                  0.659\n                  0.6\n                  -0.367\n                  0.232\n                \n        \n      \n    \n\n\n# Compare these gender-specific estimates\navg_comparisons(mod, variables = \"partner\", by = \"gender\", hypothesis = \"b2 - b1 = 0\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b2-b1=0\n                  0.00625\n                  0.18\n                  0.0348\n                  0.972\n                  0.0\n                  -0.346\n                  0.359\n                \n        \n      \n    \n\n\n# For the sake of completeness, we may also generate age-specific estimates\n\n# Note that the wiggliness of this line will depend on how we modeled age\ncomp &lt;- avg_comparisons(mod, variables = \"partner\", by = \"age\")\n\nggplot(comp, aes(x = age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() +\n  geom_ribbon(alpha = .1) +\n  geom_hline(yintercept = 0) +\n  ylab(\"Association partner and friendship importance\")"
  },
  {
    "objectID": "examples/relationship.html#ordinal-robustness-check",
    "href": "examples/relationship.html#ordinal-robustness-check",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "So far, we have simply conducted linear regressions, but that may be considered suspect given the nature of the outcome: it’s just a five-point response scale ranging from not important at all to very important. And, in fact, barely anybody used the lower response options—more than 40% picked the highest response option. This results in a distribution for which the assumptions of linear regression may be considered unrealistic.\nSo, let’s run an ordinal regression to see whether conclusions change.\nHere, we are going to fit a cumulative ordinal model with a probit link using the brms package [(Bürkner, 2018)](https://cran.r-project.org/web/packages/brms/index.html}. In essence, this approach assumes a continuous, normally distributed standardized latent variable (“true” friendship_importance) which is translated into the ordinal response variable following thresholds that are estimated from the data; for example, people who score -3 or less on the standardized latent variable may report that their friends are not important at all, people who score more than that but below -2.7 may pick the second lowest response option, and so on (see Bürkner & Vuorre, 2019 for a proper introduction to these models).\nThe rest of the model specification remains unchanged:\nWe can evaluate the association of interest using the same average comparison as before:\n\navg_comparisons(mod_ord, variables = \"partner\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00132\n                  -0.00798\n                  0.0115\n                \n                \n                  2\n                  0.00338\n                  -0.00757\n                  0.0156\n                \n                \n                  3\n                  0.02108\n                  -0.01542\n                  0.0597\n                \n                \n                  4\n                  0.02925\n                  -0.00152\n                  0.0613\n                \n                \n                  5\n                  -0.05541\n                  -0.13218\n                  0.0181\n                \n        \n      \n    \n\n\n\nThe resulting output, however, differs. By default, the output shows how the probability of any response category of friendship_importance when partner changes from 0 to 1. The response categories 1 to 4 become slightly more likely with partner = 1, whereas the probability of giving the highest rating, 5, decreases by 5.5 percentage points.\nWe can also compute the comparison on the assumed underlying latent variable, which is the scale on which the model coefficients are reported in the regression output:\n\navg_comparisons(mod_ord, variables = \"partner\", type = \"link\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.148\n                  -0.364\n                  0.057\n                \n        \n      \n    \n\n\n\nAnd, as promised in the main text, we can combine these category-specific estimates on the averaged scale.\n\n# Take the response probabilities and attach the integers 1:5 to the consecutive\n# response categories to arrive at the same metric as the linear model\navg_comparisons(mod_ord, variables = \"partner\", hypothesis = ~ I(sum(x * 1:5)))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.0883\n                  -0.244\n                  0.062\n                \n        \n      \n    \n\n\n\nWhile this ensures that we ask the ordinal model precisely the same answer that we asked the linear model, it may appear a bit inconsistent given that we usually use ordinal models precisely because we don’t want to assign integer values to the response categories – although note that here we do it only in the very last step, so the model itself does not assume that the distances between adjacent response categories are the same.\n\n\n\n# Outcome as factor variable which is what clm() expects\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\n\n# Fit the model\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\n\n# Evaluate central comparison in terms of change in response probabilities\navg_comparisons(mod_ord, variables = \"partner\") \n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00170\n                  0.00436\n                  0.390\n                  0.6963\n                  0.5\n                  -0.00685\n                  0.0103\n                \n                \n                  2\n                  0.00353\n                  0.00555\n                  0.636\n                  0.5247\n                  0.9\n                  -0.00735\n                  0.0144\n                \n                \n                  3\n                  0.02129\n                  0.01963\n                  1.085\n                  0.2780\n                  1.8\n                  -0.01718\n                  0.0598\n                \n                \n                  4\n                  0.02896\n                  0.01641\n                  1.765\n                  0.0776\n                  3.7\n                  -0.00320\n                  0.0611\n                \n                \n                  5\n                  -0.05549\n                  0.03847\n                  -1.442\n                  0.1492\n                  2.7\n                  -0.13089\n                  0.0199\n                \n        \n      \n    \n\n\n# Evaluate effect on the underlying latent variable\navg_comparisons(mod_ord, variables = \"partner\", type = \"linear.predictor\") \n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  2\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  3\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  4\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  5\n                  0.147\n                  0.107\n                  1.37\n                  0.170\n                  2.6\n                  -0.0627\n                  0.356\n                \n        \n      \n    \n\n\n# Note that this estimate has the wrong sign\n# Likely to an upstream bug in clm()\n\n# Manually reconstruct\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\nd0 &lt;- transform(dat, partner = 0)\nd1 &lt;- transform(dat, partner = 1)\np0 &lt;- predict(mod_ord, newdata = d0, type = \"linear.predictor\")\np1 &lt;- predict(mod_ord, newdata = d1, type = \"linear.predictor\")\nmean(p1$eta1 - p0$eta1)\n\n[1] 0.1465469\n\n# If we do this manually, we also get the wrong sign\n# So something seems wrong about the predictions that clm returns"
  },
  {
    "objectID": "examples/relationship.html#varying-the-complexity-of-the-linear-model-to-see-what-happens",
    "href": "examples/relationship.html#varying-the-complexity-of-the-linear-model-to-see-what-happens",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "Let’s vary model complexity to see what happens to our target quantity, the association between having a partner and friendship importance (holding constant age and gender).\n\n\n\n# No age in the model\nmod_age_0 &lt;- lm(friendship_importance ~ gender + partner +\n            partner:gender, data = dat)\ncomp_age_0 &lt;- avg_comparisons(mod_age_0, variables = \"partner\")\ncomp_age_0$model &lt;- \"Not included\"\n\n# Linear age in the model\nmod_age_1 &lt;- lm(friendship_importance ~ age + gender + partner +\n            age:gender + partner:gender + age:partner, data = dat)\ncomp_age_1 &lt;- avg_comparisons(mod_age_1, variables = \"partner\")\ncomp_age_1$model &lt;- \"Linear\"\n\n# Quadratic age on top\nmod_age_2 &lt;- lm(friendship_importance ~ poly(age, 2) + gender + partner +\n            poly(age, 2):gender + partner:gender + poly(age, 2):partner, data = dat)\ncomp_age_2 &lt;- avg_comparisons(mod_age_2, variables = \"partner\")\ncomp_age_2$model &lt;- \"Quadratic\"\n\n# Cubic age on top\nmod_age_3 &lt;- lm(friendship_importance ~ poly(age, 3) + gender + partner +\n            poly(age, 3):gender + partner:gender + poly(age, 3):partner, data = dat)\ncomp_age_3 &lt;- avg_comparisons(mod_age_3, variables = \"partner\")\ncomp_age_3$model &lt;- \"Cubic\"\n\n# Quartic age on tope\nmod_age_4 &lt;- lm(friendship_importance ~ poly(age, 4) + gender + partner +\n            poly(age, 4):gender + partner:gender + poly(age, 4):partner, data = dat)\ncomp_age_4 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_4$model &lt;- \"Quartic\"\n\n# Quintic age on tope\nmod_age_5 &lt;- lm(friendship_importance ~ poly(age, 5) + gender + partner +\n            poly(age, 5):gender + partner:gender + poly(age, 5):partner, data = dat)\ncomp_age_5 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_5$model &lt;- \"Quintic\"\n\n\n# Age with splines, df = 3\nmod_age_6 &lt;- lm(friendship_importance ~ bs(age, df = 3) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_6 &lt;- avg_comparisons(mod_age_6, variables = \"partner\")\ncomp_age_6$model &lt;- \"Splines, df = 3\"\n\n\n# Splines, df = 4 (same as model reported in main text)\nmod_age_7 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_7 &lt;- avg_comparisons(mod_age_7, variables = \"partner\")\ncomp_age_7$model &lt;- \"Splines, df = 4\"\n\n# Splines, df = 5\nmod_age_8 &lt;- lm(friendship_importance ~ bs(age, df = 5) + gender + partner +\n                  bs(age, df = 5):gender + partner:gender + bs(age, df = 5):partner, data = dat)\ncomp_age_8 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_8$model &lt;- \"Splines, df = 5\"\n\n# Splines, df = 6\nmod_age_9 &lt;- lm(friendship_importance ~ bs(age, df = 6) + gender + partner +\n                  bs(age, df = 6):gender + partner:gender + bs(age, df = 6):partner, data = dat)\ncomp_age_9 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_9$model &lt;- \"Splines, df = 6\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_age &lt;- rbind(data.frame(comp_age_0), data.frame(comp_age_1), \n                  data.frame(comp_age_2), data.frame(comp_age_3),\n                  data.frame(comp_age_4), data.frame(comp_age_5),\n                  data.frame(comp_age_6), data.frame(comp_age_7),\n                  data.frame(comp_age_8), data.frame(comp_age_9))\n\n# Numbering (for plotting purposes)\ncomp_age$no &lt;- 1:nrow(comp_age)\n\n# Generate the plot\nggplot(comp_age, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_age$no, labels = comp_age$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Way that age is included in model\") +\n  ylab(\"Target quantity (95% CI)\")\n\n\n\n# ggsave(here(\"plots/age_robustness.png\"), width = 4, height = 3)\n\n\n\n\nLet’s see whether the inclusion of interactions makes a difference\n\n# No interactions\nmod_interact_1 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner, data = dat)\ncomp_interact_1 &lt;- avg_comparisons(mod_interact_1, variables = \"partner\")\ncomp_interact_1$model &lt;- \"No interaction\"\n\n# Add two-way interactions (same as model reported in main text)\nmod_interact_2 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                       bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\ncomp_interact_2 &lt;- avg_comparisons(mod_interact_2, variables = \"partner\")\ncomp_interact_2$model &lt;- \"Two-way interactions\"\n\n# Add three-way interaction\nmod_interact_3 &lt;- lm(friendship_importance ~ bs(age, df = 4)*gender*partner, data = dat)\ncomp_interact_3 &lt;- avg_comparisons(mod_interact_3, variables = \"partner\")\ncomp_interact_3$model &lt;- \"Three-way interaction\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_interact &lt;- rbind(data.frame(comp_interact_1), data.frame(comp_interact_2), data.frame(comp_interact_3))\n\n# Numbering (for plotting purposes)\ncomp_interact$no &lt;- 1:nrow(comp_interact)\n\n# Plot the results\nggplot(comp_interact, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_interact$no, labels = comp_interact$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Interactions included\") +\n  ylab(\"Estimated difference in friendship importance\")"
  },
  {
    "objectID": "examples/age.html",
    "href": "examples/age.html",
    "title": "Age trajectory",
    "section": "",
    "text": "source(here::here(\"scripts/load.R\"))\n\n# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)\n\n\n\n\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\n\n# Expand categorical predictions to plot them as step function including ribbon\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# color\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#CC79A7\"\n\n# Two extremes\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, size = .2, linetype = \"dashed\") +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\")\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_extremes.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\n# Smooth solutions\n\n# Splines\nage_splines &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\npred_splines &lt;- avg_predictions(age_splines, by = \"age\")\n\n# Polynomial\nage_poly &lt;- lm(friendship_importance ~ I(age^4) + I(age^3) + I(age^2) + age, data = dat)\npred_poly &lt;- avg_predictions(age_poly, by = \"age\")\n\n# Bins\ncutoffs &lt;- quantile(dat$age, probs = seq(0, 1, length.out = 6), na.rm = TRUE)\n\nage_bin &lt;- lm(friendship_importance ~ I(age &lt;= 22) + \n                I(age &gt; 22 & age &lt;= 29) + \n                I(age &gt; 29 & age &lt;= 39) + \n                I(age &gt; 39 & age &lt;= 50), data = dat)\npred_bin &lt;- avg_predictions(age_bin, by = \"age\")\n\ncol_neutral_1 &lt;- \"darkgrey\"\ncol_neutral_2 &lt;- \"lightgrey\"\ncol_smooth &lt;- \"#0072B2\"\n\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_splines, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_splines.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_poly, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_poly.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_bin, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_bin, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_bin.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\ncol_splines &lt;- \"#009E73\"\n\n# Disagreement: poly and splines\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_splines)"
  },
  {
    "objectID": "examples/age.html#visualize-associations-with-age",
    "href": "examples/age.html#visualize-associations-with-age",
    "title": "Age trajectory",
    "section": "",
    "text": "age_lin &lt;- lm(friendship_importance ~ age, data = dat)\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\n\n# Expand categorical predictions to plot them as step function including ribbon\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# color\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#CC79A7\"\n\n# Two extremes\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, size = .2, linetype = \"dashed\") +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\")\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_extremes.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\n# Smooth solutions\n\n# Splines\nage_splines &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\npred_splines &lt;- avg_predictions(age_splines, by = \"age\")\n\n# Polynomial\nage_poly &lt;- lm(friendship_importance ~ I(age^4) + I(age^3) + I(age^2) + age, data = dat)\npred_poly &lt;- avg_predictions(age_poly, by = \"age\")\n\n# Bins\ncutoffs &lt;- quantile(dat$age, probs = seq(0, 1, length.out = 6), na.rm = TRUE)\n\nage_bin &lt;- lm(friendship_importance ~ I(age &lt;= 22) + \n                I(age &gt; 22 & age &lt;= 29) + \n                I(age &gt; 29 & age &lt;= 39) + \n                I(age &gt; 39 & age &lt;= 50), data = dat)\npred_bin &lt;- avg_predictions(age_bin, by = \"age\")\n\ncol_neutral_1 &lt;- \"darkgrey\"\ncol_neutral_2 &lt;- \"lightgrey\"\ncol_smooth &lt;- \"#0072B2\"\n\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_splines, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_splines.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_poly, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_poly.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_bin, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_bin, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_bin.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\ncol_splines &lt;- \"#009E73\"\n\n# Disagreement: poly and splines\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_splines)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models as Prediction Machines",
    "section": "",
    "text": "This website hosts a replication package detailed worked examples to accompany this article:\n\n“Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities” by Julia M. Rohrer and Vincent Arel-Bundock (2025)\n\nYou can find the article on: Insert Website Address\nWorked Examples:\n\nIllustrating target quantities\nRelationship Status and the Importance of Friends\nFriends by Chance\n\nDownload the full replication package here.\nThe material on this site is licensed under a Creative Commons Attribution 4.0 International License. This means that you can freely share and adapt the material, as long as you cite the article listed above."
  },
  {
    "objectID": "index.html#how-to-convert-confusing-coefficients-into-clear-quantities",
    "href": "index.html#how-to-convert-confusing-coefficients-into-clear-quantities",
    "title": "Models as Prediction Machines",
    "section": "",
    "text": "This website hosts a replication package detailed worked examples to accompany this article:\n\n“Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities” by Julia M. Rohrer and Vincent Arel-Bundock (2025)\n\nYou can find the article on: Insert Website Address\nWorked Examples:\n\nIllustrating target quantities\nRelationship Status and the Importance of Friends\nFriends by Chance\n\nDownload the full replication package here.\nThe material on this site is licensed under a Creative Commons Attribution 4.0 International License. This means that you can freely share and adapt the material, as long as you cite the article listed above."
  },
  {
    "objectID": "examples/soep.html",
    "href": "examples/soep.html",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "Before we start, let’s execute a helper script that loads the necessary dependencies.\n\nsource(here::here(\"scripts/load.R\"))\n\n\n\nThis document contains the analysis code underlying the running example in the manuscript section on target quantities, as well as the code that generates Figure 1. Annotations were later added manually using PowerPoint.\n\n\n\nWe analyze freely available practice data from the German Socio-Economic Panel Study (SOEP). The data can be downloaded from the SOEP website (https://www.diw.de/en/diw_01.c.836543.en/soep_practice_dataset.html). We are using the English version (DOI: 10.5684/soep.practice.v36).\n\nsoep &lt;- read_dta(here(\"data/practice_en/practice_dataset_eng.dta\"))\nhead(soep)\n\n# A tibble: 6 × 15\n     id syear sex            alter anz_pers anz_kind bildung   erwerb                      branche                                                                 gesund_org           lebensz_org einkommenj1 einkommenj2 einkommenm1 einkommenm2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;                   &lt;dbl+lbl&gt;                                                               &lt;dbl+lbl&gt;            &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;  \n1   194  2015 1 [[1] female]    59        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 4 [[4] Not so well]  6           28679.      0           1659.       0          \n2   194  2016 1 [[1] female]    60        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 3 [[3] Satisfactory] 5           19962.      0           1809.       0          \n3   194  2017 1 [[1] female]    61        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 3 [[3] Satisfactory] 7           22228.      0           1849.       0          \n4   194  2018 1 [[1] female]    62        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 5 [[5] Badly]        5           22100.      0           1617.       0          \n5   194  2019 1 [[1] female]    63        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 4 [[4] Not so well]  6           23158.      0           1901.       0          \n6 19052  2015 1 [[1] female]    74        2        0 10        5 [[5] Not employed]        NA                                                                      2 [[2] Well]         8               0       0              0        0          \n\n\n\n\n\nWe fit a flexible model that predicts life satisfaction based on sex, age, and income. The model uses basis splines for age and income to capture non-linear relationships and includes all interactions between variables. We limit our analysis to people under the age of 60.\n\nmod &lt;- lm(\n  lebensz_org ~ sex * bs(alter, df = 3) * bs(einkommenj1, df = 3),\n  data = soep[soep$alter &lt; 60, ]\n)\n\nLet’s examine the model coefficients to confirm they are not easily interpretable in their raw form:\n\nsummary(mod)\n\n\nCall:\nlm(formula = lebensz_org ~ sex * bs(alter, df = 3) * bs(einkommenj1, \n    df = 3), data = soep[soep$alter &lt; 60, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8030 -0.6359  0.3642  1.0181  3.2115 \n\nCoefficients:\n                                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                                        7.8621     0.1032  76.148  &lt; 2e-16 ***\nsex                                               -0.3415     0.1380  -2.475  0.01332 *  \nbs(alter, df = 3)1                                -1.2071     0.3793  -3.183  0.00146 ** \nbs(alter, df = 3)2                                -1.1611     0.2675  -4.341 1.43e-05 ***\nbs(alter, df = 3)3                                -0.9721     0.1881  -5.169 2.39e-07 ***\nbs(einkommenj1, df = 3)1                          -2.9110     1.9740  -1.475  0.14031    \nbs(einkommenj1, df = 3)2                          19.8332    19.4858   1.018  0.30877    \nbs(einkommenj1, df = 3)3                          -0.8611   132.6094  -0.006  0.99482    \nsex:bs(alter, df = 3)1                             1.2525     0.4806   2.606  0.00917 ** \nsex:bs(alter, df = 3)2                             0.5992     0.3352   1.788  0.07380 .  \nsex:bs(alter, df = 3)3                             0.4095     0.2492   1.643  0.10035    \nsex:bs(einkommenj1, df = 3)1                      -2.6841     3.2689  -0.821  0.41160    \nsex:bs(einkommenj1, df = 3)2                      65.7747    40.8548   1.610  0.10743    \nsex:bs(einkommenj1, df = 3)3                    -457.9446   394.2805  -1.161  0.24547    \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)1        8.7901     3.9310   2.236  0.02536 *  \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)1        3.1041     2.3496   1.321  0.18649    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)1        3.9785     2.2388   1.777  0.07558 .  \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)2      -27.3680    28.4171  -0.963  0.33552    \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)2      -12.4798    18.6404  -0.670  0.50319    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)2      -17.4873    20.0521  -0.872  0.38317    \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)3      -21.9448   181.2849  -0.121  0.90365    \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)3        8.6725   121.1156   0.072  0.94292    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)3       -1.0620   135.1902  -0.008  0.99373    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)1    2.9322     6.7533   0.434  0.66415    \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)1    4.4830     3.6165   1.240  0.21514    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)1    1.8393     3.9502   0.466  0.64149    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)2 -124.9928    65.3625  -1.912  0.05585 .  \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)2  -56.0276    36.8202  -1.522  0.12812    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)2  -68.1158    44.7625  -1.522  0.12810    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)3  910.8846   570.4201   1.597  0.11031    \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)3  256.8281   346.2656   0.742  0.45827    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)3  528.8416   415.4596   1.273  0.20307    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.63 on 16260 degrees of freedom\n  (589 observations deleted due to missingness)\nMultiple R-squared:  0.0339,    Adjusted R-squared:  0.03205 \nF-statistic:  18.4 on 31 and 16260 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nNow we’ll demonstrate different types of marginal effects calculations using a hypothetical 35-year-old woman as our example.\n\n\nFirst, let’s predict the life satisfaction of a 35-year-old woman who earns €20,000:\n\nprediction_20000 &lt;- predictions(mod, \n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000)\n)\nprediction_20000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  7.65\n                  0.0489\n                  156\n                  &lt;0.001\n                  Inf\n                  7.55\n                  7.74\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nWhat if she earned twice as much (€40,000)?\n\nprediction_40000 &lt;- predictions(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 40000)\n)\nprediction_40000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  7.77\n                  0.0683\n                  114\n                  &lt;0.001\n                  Inf\n                  7.64\n                  7.91\n                  1\n                  35\n                  40000\n                \n        \n      \n    \n\n\n\n\n\n\nWe can directly compare these two predictions to see the effect of doubling income:\n\ncmp &lt;- comparisons(\n  mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = list(\"einkommenj1\" = c(20000, 40000))\n)\ncmp\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  0.127\n                  0.07\n                  1.81\n                  0.0697\n                  3.8\n                  -0.0102\n                  0.264\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nIncreasing income from €20,000 to €40,000 results in an increase of approximately {r} sprintf(\"%.3f\", cmp$estimate) points in life satisfaction.\n\n\n\nHow much does life satisfaction increase with income at the €20,000 level?\n\nslope_20000 &lt;- slopes(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = \"einkommenj1\"\n)\nslope_20000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  1e-05\n                  2.65e-06\n                  3.8\n                  &lt;0.001\n                  12.7\n                  4.86e-06\n                  1.52e-05\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nThe slope is interesting, but it is often more useful to express the association between the outcome and the predictor in terms of a discrete change. For this, we can use the variables argument in the comparisons function.\n\ncmp &lt;- comparisons(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = list(\"einkommenj1\" = 10000)\n)\ncmp\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  0.0786\n                  0.0311\n                  2.53\n                  0.0116\n                  6.4\n                  0.0176\n                  0.14\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nOur model suggests that increasing income by €10,000, starting from the €20,000 level, is associated with an increase in life satisfaction of approximately {r} sprintf(\"%.3f\", cmp$estimate) points.\n\n\n\n\nIn the main manuscript, we illustrate the different quantities calculated above in Figure 1. This figure shows the predictions, comparisons, and slopes that characterize the estimated relationship between income and life satisfaction for a specific type of individual.\n\n\nWe could directly create a plot using the marginaleffects package:\n\nlibrary(patchwork)\n\nplot_predictions(mod, condition = list(\n  einkommenj1 = 0:80000,\n  sex = 1,\n  alter = 35))\n\n\n\n\n\n\n\nHowever, for better control over the visualization, let’s create a custom plot that illustrates the different types of quantities we calculated above.\nFirst, we generate predictions across a range of income values for our hypothetical 35-year-old woman:\n\nhypothetical_woman &lt;- data.frame(\n  sex = 1,\n  alter = 35,\n  einkommenj1 = 0:80000\n)\n\nhypothetical_predictions &lt;- predictions(mod, newdata = hypothetical_woman)\nhead(hypothetical_predictions)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n        \n      \n    \n\n\nhypothetical_woman$lebensz_org &lt;- hypothetical_predictions$estimate\n\nNow we set up the plot styling:\n\nslope_range &lt;- c(0, 60000)\n\ncolor_prediction &lt;- \"#0072B2\"\ncolor_comparison &lt;- \"#56B4E9\"\ncolor_slope &lt;- \"#D55E00\"\ncolor_neutral &lt;- \"#BBBBBB\"\n\nFinally, we create the plot that visualizes predictions, comparisons, and slopes:\n\nggplot(data = hypothetical_woman, aes(x = einkommenj1, y = lebensz_org)) +\n  geom_line() +\n  # Comparison 20,000 vs 40,000 (connected with  segments that form a slope triangle)\n  geom_segment(\n    x = 20000,\n    xend = 40000,\n    y = prediction_20000$estimate,\n    color = color_neutral,\n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    x = 40000,\n    y = prediction_20000$estimate,\n    yend = prediction_40000$estimate,\n    color = color_comparison,\n    linetype = \"solid\"\n  ) +\n  # Slope line at an income of 20,000\n  geom_segment(\n    x = slope_range[1],\n    xend = slope_range[2],\n    y = prediction_20000$estimate - (20000 - slope_range[1]) * slope_20000$estimate,\n    yend = prediction_20000$estimate + (slope_range[2] - 20000) * slope_20000$estimate,\n    color = color_slope\n  ) +\n  # Predictions at 20,000 and 40,000\n  geom_point(x = 20000, y = prediction_20000$estimate, color = color_prediction) +\n  geom_point(x = 40000, y = prediction_40000$estimate, color = color_prediction) +\n  # Layout, make it look nicer\n  xlab(\"Annual gross income\") +\n  ylab(\"Predicted life satisfaction\") +\n  coord_cartesian(ylim = c(7, 8.25)) +\n  scale_x_continuous(\n    labels = label_dollar(prefix = \"\", suffix = \"€\", big.mark = \",\")\n  )"
  },
  {
    "objectID": "examples/soep.html#overview",
    "href": "examples/soep.html#overview",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "This document contains the analysis code underlying the running example in the manuscript section on target quantities, as well as the code that generates Figure 1. Annotations were later added manually using PowerPoint."
  },
  {
    "objectID": "examples/soep.html#data-source",
    "href": "examples/soep.html#data-source",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "We analyze freely available practice data from the German Socio-Economic Panel Study (SOEP). The data can be downloaded from the SOEP website (https://www.diw.de/en/diw_01.c.836543.en/soep_practice_dataset.html). We are using the English version (DOI: 10.5684/soep.practice.v36).\n\nsoep &lt;- read_dta(here(\"data/practice_en/practice_dataset_eng.dta\"))\nhead(soep)\n\n# A tibble: 6 × 15\n     id syear sex            alter anz_pers anz_kind bildung   erwerb                      branche                                                                 gesund_org           lebensz_org einkommenj1 einkommenj2 einkommenm1 einkommenm2\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl+lbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl+lbl&gt; &lt;dbl+lbl&gt;                   &lt;dbl+lbl&gt;                                                               &lt;dbl+lbl&gt;            &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;   &lt;dbl+lbl&gt;  \n1   194  2015 1 [[1] female]    59        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 4 [[4] Not so well]  6           28679.      0           1659.       0          \n2   194  2016 1 [[1] female]    60        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 3 [[3] Satisfactory] 5           19962.      0           1809.       0          \n3   194  2017 1 [[1] female]    61        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 3 [[3] Satisfactory] 7           22228.      0           1849.       0          \n4   194  2018 1 [[1] female]    62        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 5 [[5] Badly]        5           22100.      0           1617.       0          \n5   194  2019 1 [[1] female]    63        2        0 10.5      2 [[-2] Employed part-time] 84 [[84] Public administration and defense; compulsory social security] 4 [[4] Not so well]  6           23158.      0           1901.       0          \n6 19052  2015 1 [[1] female]    74        2        0 10        5 [[5] Not employed]        NA                                                                      2 [[2] Well]         8               0       0              0        0"
  },
  {
    "objectID": "examples/soep.html#model-specification",
    "href": "examples/soep.html#model-specification",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "We fit a flexible model that predicts life satisfaction based on sex, age, and income. The model uses basis splines for age and income to capture non-linear relationships and includes all interactions between variables. We limit our analysis to people under the age of 60.\n\nmod &lt;- lm(\n  lebensz_org ~ sex * bs(alter, df = 3) * bs(einkommenj1, df = 3),\n  data = soep[soep$alter &lt; 60, ]\n)\n\nLet’s examine the model coefficients to confirm they are not easily interpretable in their raw form:\n\nsummary(mod)\n\n\nCall:\nlm(formula = lebensz_org ~ sex * bs(alter, df = 3) * bs(einkommenj1, \n    df = 3), data = soep[soep$alter &lt; 60, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.8030 -0.6359  0.3642  1.0181  3.2115 \n\nCoefficients:\n                                                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                                        7.8621     0.1032  76.148  &lt; 2e-16 ***\nsex                                               -0.3415     0.1380  -2.475  0.01332 *  \nbs(alter, df = 3)1                                -1.2071     0.3793  -3.183  0.00146 ** \nbs(alter, df = 3)2                                -1.1611     0.2675  -4.341 1.43e-05 ***\nbs(alter, df = 3)3                                -0.9721     0.1881  -5.169 2.39e-07 ***\nbs(einkommenj1, df = 3)1                          -2.9110     1.9740  -1.475  0.14031    \nbs(einkommenj1, df = 3)2                          19.8332    19.4858   1.018  0.30877    \nbs(einkommenj1, df = 3)3                          -0.8611   132.6094  -0.006  0.99482    \nsex:bs(alter, df = 3)1                             1.2525     0.4806   2.606  0.00917 ** \nsex:bs(alter, df = 3)2                             0.5992     0.3352   1.788  0.07380 .  \nsex:bs(alter, df = 3)3                             0.4095     0.2492   1.643  0.10035    \nsex:bs(einkommenj1, df = 3)1                      -2.6841     3.2689  -0.821  0.41160    \nsex:bs(einkommenj1, df = 3)2                      65.7747    40.8548   1.610  0.10743    \nsex:bs(einkommenj1, df = 3)3                    -457.9446   394.2805  -1.161  0.24547    \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)1        8.7901     3.9310   2.236  0.02536 *  \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)1        3.1041     2.3496   1.321  0.18649    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)1        3.9785     2.2388   1.777  0.07558 .  \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)2      -27.3680    28.4171  -0.963  0.33552    \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)2      -12.4798    18.6404  -0.670  0.50319    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)2      -17.4873    20.0521  -0.872  0.38317    \nbs(alter, df = 3)1:bs(einkommenj1, df = 3)3      -21.9448   181.2849  -0.121  0.90365    \nbs(alter, df = 3)2:bs(einkommenj1, df = 3)3        8.6725   121.1156   0.072  0.94292    \nbs(alter, df = 3)3:bs(einkommenj1, df = 3)3       -1.0620   135.1902  -0.008  0.99373    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)1    2.9322     6.7533   0.434  0.66415    \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)1    4.4830     3.6165   1.240  0.21514    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)1    1.8393     3.9502   0.466  0.64149    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)2 -124.9928    65.3625  -1.912  0.05585 .  \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)2  -56.0276    36.8202  -1.522  0.12812    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)2  -68.1158    44.7625  -1.522  0.12810    \nsex:bs(alter, df = 3)1:bs(einkommenj1, df = 3)3  910.8846   570.4201   1.597  0.11031    \nsex:bs(alter, df = 3)2:bs(einkommenj1, df = 3)3  256.8281   346.2656   0.742  0.45827    \nsex:bs(alter, df = 3)3:bs(einkommenj1, df = 3)3  528.8416   415.4596   1.273  0.20307    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.63 on 16260 degrees of freedom\n  (589 observations deleted due to missingness)\nMultiple R-squared:  0.0339,    Adjusted R-squared:  0.03205 \nF-statistic:  18.4 on 31 and 16260 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "examples/soep.html#calculating-various-quantities",
    "href": "examples/soep.html#calculating-various-quantities",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "Now we’ll demonstrate different types of marginal effects calculations using a hypothetical 35-year-old woman as our example.\n\n\nFirst, let’s predict the life satisfaction of a 35-year-old woman who earns €20,000:\n\nprediction_20000 &lt;- predictions(mod, \n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000)\n)\nprediction_20000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  7.65\n                  0.0489\n                  156\n                  &lt;0.001\n                  Inf\n                  7.55\n                  7.74\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nWhat if she earned twice as much (€40,000)?\n\nprediction_40000 &lt;- predictions(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 40000)\n)\nprediction_40000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  7.77\n                  0.0683\n                  114\n                  &lt;0.001\n                  Inf\n                  7.64\n                  7.91\n                  1\n                  35\n                  40000\n                \n        \n      \n    \n\n\n\n\n\n\nWe can directly compare these two predictions to see the effect of doubling income:\n\ncmp &lt;- comparisons(\n  mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = list(\"einkommenj1\" = c(20000, 40000))\n)\ncmp\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  0.127\n                  0.07\n                  1.81\n                  0.0697\n                  3.8\n                  -0.0102\n                  0.264\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nIncreasing income from €20,000 to €40,000 results in an increase of approximately {r} sprintf(\"%.3f\", cmp$estimate) points in life satisfaction.\n\n\n\nHow much does life satisfaction increase with income at the €20,000 level?\n\nslope_20000 &lt;- slopes(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = \"einkommenj1\"\n)\nslope_20000\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  1e-05\n                  2.65e-06\n                  3.8\n                  &lt;0.001\n                  12.7\n                  4.86e-06\n                  1.52e-05\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nThe slope is interesting, but it is often more useful to express the association between the outcome and the predictor in terms of a discrete change. For this, we can use the variables argument in the comparisons function.\n\ncmp &lt;- comparisons(mod,\n  newdata = data.frame(sex = 1, alter = 35, einkommenj1 = 20000),\n  variables = list(\"einkommenj1\" = 10000)\n)\ncmp\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n                sex\n                alter\n                einkommenj1\n              \n        \n        \n        \n                \n                  0.0786\n                  0.0311\n                  2.53\n                  0.0116\n                  6.4\n                  0.0176\n                  0.14\n                  1\n                  35\n                  20000\n                \n        \n      \n    \n\n\n\nOur model suggests that increasing income by €10,000, starting from the €20,000 level, is associated with an increase in life satisfaction of approximately {r} sprintf(\"%.3f\", cmp$estimate) points."
  },
  {
    "objectID": "examples/soep.html#generate-figure-1",
    "href": "examples/soep.html#generate-figure-1",
    "title": "Illustrating target quantities",
    "section": "",
    "text": "In the main manuscript, we illustrate the different quantities calculated above in Figure 1. This figure shows the predictions, comparisons, and slopes that characterize the estimated relationship between income and life satisfaction for a specific type of individual.\n\n\nWe could directly create a plot using the marginaleffects package:\n\nlibrary(patchwork)\n\nplot_predictions(mod, condition = list(\n  einkommenj1 = 0:80000,\n  sex = 1,\n  alter = 35))\n\n\n\n\n\n\n\nHowever, for better control over the visualization, let’s create a custom plot that illustrates the different types of quantities we calculated above.\nFirst, we generate predictions across a range of income values for our hypothetical 35-year-old woman:\n\nhypothetical_woman &lt;- data.frame(\n  sex = 1,\n  alter = 35,\n  einkommenj1 = 0:80000\n)\n\nhypothetical_predictions &lt;- predictions(mod, newdata = hypothetical_woman)\nhead(hypothetical_predictions)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n                \n                  7.32\n                  0.0447\n                  164\n                  &lt;0.001\n                  Inf\n                  7.23\n                  7.41\n                \n        \n      \n    \n\n\nhypothetical_woman$lebensz_org &lt;- hypothetical_predictions$estimate\n\nNow we set up the plot styling:\n\nslope_range &lt;- c(0, 60000)\n\ncolor_prediction &lt;- \"#0072B2\"\ncolor_comparison &lt;- \"#56B4E9\"\ncolor_slope &lt;- \"#D55E00\"\ncolor_neutral &lt;- \"#BBBBBB\"\n\nFinally, we create the plot that visualizes predictions, comparisons, and slopes:\n\nggplot(data = hypothetical_woman, aes(x = einkommenj1, y = lebensz_org)) +\n  geom_line() +\n  # Comparison 20,000 vs 40,000 (connected with  segments that form a slope triangle)\n  geom_segment(\n    x = 20000,\n    xend = 40000,\n    y = prediction_20000$estimate,\n    color = color_neutral,\n    linetype = \"dashed\"\n  ) +\n  geom_segment(\n    x = 40000,\n    y = prediction_20000$estimate,\n    yend = prediction_40000$estimate,\n    color = color_comparison,\n    linetype = \"solid\"\n  ) +\n  # Slope line at an income of 20,000\n  geom_segment(\n    x = slope_range[1],\n    xend = slope_range[2],\n    y = prediction_20000$estimate - (20000 - slope_range[1]) * slope_20000$estimate,\n    yend = prediction_20000$estimate + (slope_range[2] - 20000) * slope_20000$estimate,\n    color = color_slope\n  ) +\n  # Predictions at 20,000 and 40,000\n  geom_point(x = 20000, y = prediction_20000$estimate, color = color_prediction) +\n  geom_point(x = 40000, y = prediction_40000$estimate, color = color_prediction) +\n  # Layout, make it look nicer\n  xlab(\"Annual gross income\") +\n  ylab(\"Predicted life satisfaction\") +\n  coord_cartesian(ylim = c(7, 8.25)) +\n  scale_x_continuous(\n    labels = label_dollar(prefix = \"\", suffix = \"€\", big.mark = \",\")\n  )"
  },
  {
    "objectID": "examples/deskmates.html",
    "href": "examples/deskmates.html",
    "title": "Friends by Chance",
    "section": "",
    "text": "Before we start, let’s execute a helper script that loads the necessary dependencies.\n\nsource(here::here(\"scripts/load.R\"))\n\n\n\nThis document contains example 3, in which we analyze the effect of being seated next to each other on friendships, using Bayesian multilevel models. Data can be downloaded from the OSF but are also included in the downloadable replication package.\nEvery day experience—and previous research—suggests that being spatially close to others can result in friendships. But does spatial proximity also lead to friendships for people who are quite different from each other? We re-analyze data from a large field experiment conducted in 3rd to 8th grade classrooms in rural Hungary previously reported in Rohrer et al. (2021). Proximity was experimentally manipulated by randomizing each classrooms’ seating chart at the beginning of the school semester; thus, students randomly ended up next to each other (deskmate = 1) or not (deskmate = 0). At the end of the semester, students listed up to five best friends from their classroom, which allows us to determine which pairs of students had formed friendships (friendship = 1). Additionally, we know students’ gender and grade point average before the experiment (GPA), which allows us to investigate whether proximity also “works” for girls seated next to boys (who are quite unlikely to befriend each other at that age) or students with discrepant levels of academic achievement.\n\n\n\n\n# Read the data\ndat &lt;- read.csv(here(\"data/deskmates.csv\"))\n\n# Arbitrary subset of 60 classes to make model-fitting less tedious\ndat &lt;- dat[dat$class_id %in% unique(dat$class_id)[1:60], ]\n\n# Keep complete cases\ndat &lt;- dat[complete.cases(dat),]\n\n\n# Label focal variables\ndat &lt;- transform(dat,\n    gender_combination = factor(girl_match, label = c(\"Boys\", \"Mixed\", \"Girls\")),\n    deskmate = factor(deskmate, label = c(\"Different desk\", \"Same desk\")))\n\n# Rename to match labels in manuscript\ndat &lt;- dat |&gt;\n  rename(GPA_average = mean_gpa,\n         GPA_difference = diff_gpa,\n         friendship = friend,\n         classroom = class_id,\n         student1 = s1,\n         student2 = s2)\n\n\n# Display the first few rows of data\nhead(dat)\n\n  any_single       deskmate sim_s1_s2 girl_s1 girl_s2 roma_s1 roma_s2 gpa_s1 gpa_s2 friendship friend_s1 friend_s2 girl_match roma_match GPA_average GPA_difference sim_s1_s2_std size size.1 gendermatch classroom school_id student1 student2 gender_combination\n1          0      Same desk 0.6500000       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.2570821   18     18           0     59379        32   731891   773914              Mixed\n2          0 Different desk 0.9566667       1       1       0       0    4.4    5.0          1         1         1          2          0         4.7            0.6     1.0919846   18     18           1     59379        32   731891   492505              Girls\n3          0 Different desk 0.6366667       1       0       0       0    4.4    4.0          0         0         0          1          0         4.2            0.4    -0.3157372   18     18           0     59379        32   731891   300996              Mixed\n4          0 Different desk 0.6233333       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.3743922   18     18           0     59379        32   731891   888954              Mixed\n5          0 Different desk 0.9833333       1       1       0       0    4.4    4.6          1         1         1          2          0         4.5            0.2     1.2092947   18     18           1     59379        32   731891   706146              Girls\n6          0 Different desk 0.6233333       1       0       0       0    4.4    5.0          0         0         0          1          0         4.7            0.6    -0.3743922   18     18           0     59379        32   731891   288809              Mixed\n\n\n\n\n\nOur model here ends up a bit more complex due to the nested structure of the data, and we use the brms package (Bürkner, 2018) which allows us to fit multilevel models in a highly flexible manner. Since we now fit a Bayesian model (relying on the default priors provided by the package), marginaleffects will return credible intervals rather than confidence intervals. Our unit of observation is pairs of students, which are nested within students and classrooms. For each pair, we know: * whether they are deskmates * their gender_combination (both boys, one girl and one boy, both girls) * their GPA_average (i.e., the average across both students) and their absolute GPA_difference (i.e., the discrepancy between both students) * whether they report a friendship at the end of the experiment or not\nHere, we fit and save the model to avoid refitting repeatedly.\n\nmod &lt;- brm(\n    friendship ~ deskmate + gender_combination + deskmate:gender_combination +\n        GPA_average + GPA_difference + GPA_average:deskmate + GPA_difference:deskmate +\n        (1 | classroom) + (1 | mm(student1, student2)),\n    family = bernoulli(link = \"logit\"),\n    data = dat,\n    seed = 12345,\n    cores = 4\n)\n\n# Save model fit\nsaveRDS(mod, file = here(\"fits/deskmates.rds\"))\n\n\n# Load the fitted model\nmod &lt;- readRDS(here(\"fits/deskmates.rds\"))\n\n# Look at the model coefficients\nparameters(mod)\n\n# Fixed Effects\n\nParameter                                |   Median |         95% CI |     pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)                              |    -1.62 | [-2.34, -0.94] |   100% | 1.000 | 1791.00\ndeskmateSamedesk                         |     0.22 | [-1.49,  1.96] | 60.02% | 1.001 | 2402.00\ngender_combinationMixed                  |    -3.54 | [-3.88, -3.23] |   100% | 1.002 | 2582.00\ngender_combinationGirls                  | 5.66e-03 | [-0.25,  0.25] | 51.75% | 1.001 | 2254.00\nGPA_average                              |     0.30 | [ 0.13,  0.49] | 99.95% | 1.001 | 1495.00\nGPA_difference                           |    -0.42 | [-0.56, -0.28] |   100% | 1.000 | 4267.00\ndeskmateSamedesk:gender_combinationMixed |     0.12 | [-0.86,  0.98] | 59.62% | 1.000 | 4483.00\ndeskmateSamedesk:gender_combinationGirls |     0.04 | [-0.69,  0.78] | 54.07% | 1.001 | 4559.00\ndeskmateSamedesk:GPA_average             |     0.18 | [-0.26,  0.62] | 79.67% | 1.000 | 2529.00\ndeskmateSamedesk:GPA_difference          |    -0.03 | [-0.60,  0.47] | 54.55% | 1.000 | 4436.00\n\n\n\n\n\n\n\nIn principle, we could evaluate this model on the log-odds scale on which the coefficients are estimated.\n\navg_comparisons(mod, variables = \"deskmate\", type = \"link\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.958\n                  0.533\n                  1.35\n                \n        \n      \n    \n\n\n\nHowever, log-odds of friendship are not a particularly intuitive unit, and so instead we may want to switch to the scale of the outcome (friendships)—which is the default behavior of marginaleffects:\n\n\n\n\n# Predicted probabilities\navg_predictions(mod, by = \"deskmate\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                deskmate\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Different desk\n                  0.166\n                  0.158\n                  0.174\n                \n                \n                  Same desk\n                  0.270\n                  0.234\n                  0.311\n                \n        \n      \n    \n\n\n# Average Treatment Effect\nate &lt;- avg_comparisons(mod, variables = \"deskmate\")\nate\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.108\n                  0.0697\n                  0.15\n                \n        \n      \n    \n\n\n\nThis returns the average effect of the intervention in percentage points.\n\n\n\nOne may want to compare the effect of sitting next to each other to other interventions meant to foster friendships. For example, one staple of psychological research is the fast friends procedure in which two participants are paired up and then take turns answering questions that escalate in the degree of self-disclosure involved, from mild (“Would you like to be famous? In what way?”) to severe (“When did you last cry in front of another person?”). Echols and Ivanich (2021) implemented such a procedure in US middle school students and found that those who underwent the intervention in three sessions over three months were 10 percentage points more likely to become friends. This seems very close to the 11 percentage point effect we observed in our analysis.\nWould it be justified to conclude that the effects are practically the same?\nIn a Frequentist framework, this would be a use case for an equivalence test. Given our Bayesian model, we instead resort to the notion of a region of practical equivalence (ROPE; Kruschke, 2018; Makowski et al., 2019. First, we need to define a range around the 10 percentage points of the fast friends procedure for which we would consider the effects equivalent for practical purposes. Here, we decide that the effect ± a quarter of the effect is a sensible range, resulting in a ROPE of [0.075; 0.125].\nNow, we can calculate how likely it is that the effect of sitting next to each other falls into the ROPE of the fast friends procedure. For this, we additionally make use of the convenience provided by the posterior package:\n\nlibrary(posterior)\ndraws &lt;- get_draws(avg_comparisons(mod, variables = \"deskmate\"), \"rvar\")\n\nPr(draws$rvar &lt; 0.125) # Probability that below the upper bound\n\n[1] 0.79775\n\nPr(draws$rvar &lt; 0.075) # Probability that below the lower bound\n\n[1] 0.04175\n\n# In combination, from this we can conclude the probability that\n# the parameter lies within the bounds\nPr(draws$rvar &lt; 0.125) - Pr(draws$rvar &lt; 0.075)\n\n[1] 0.756\n\n\n\n\n\nHaving looked at the average effect of the intervention, we still do not yet know whether being seated next to each other also “works” for dissimilar students. Here, we will keep evaluating effects on the outcome scale, which we consider most intuitive. First, we can separately calculate average effects depending on gender_combination:\n\navg_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender_combination\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Boys\n                  0.1683\n                  0.07489\n                  0.2675\n                \n                \n                  Mixed\n                  0.0317\n                  0.00534\n                  0.0684\n                \n                \n                  Girls\n                  0.1940\n                  0.08685\n                  0.3018\n                \n        \n      \n    \n\n\n\n\n\nWe can easily compare all three average effects against each other in a pairwise manner:\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = ~pairwise)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (Mixed) - (Boys)\n                  -0.1355\n                  -0.2384\n                  -0.0346\n                \n                \n                  (Girls) - (Boys)\n                  0.0273\n                  -0.1169\n                  0.1704\n                \n                \n                  (Girls) - (Mixed)\n                  0.1607\n                  0.0441\n                  0.2760\n                \n        \n      \n    \n\n\n\n\n\n\nWe may also want to compare gender-matched dyads (pairs of girls, pairs of boys) with gender-mismatched dyads (pairs of a girl and a boy), (b_Two girls + b_Two boys)/2 = b_One girl one boy. This can be achieved by using a different hypothesis argument (being mindful of the order in which the groups are listed in our output):\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = \"(b1 + b3)/2 = b2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (b1+b3)/2=b2\n                  0.149\n                  0.0667\n                  0.229\n                \n        \n      \n    \n\n\n\n\n\n\nWe can also visualize these different effects:\n\n# Plot the predicted probabilities\nplot_predictions(mod, by = c(\"gender_combination\", \"deskmate\"))\n\n\n\n# Plot the effects of the intervention in percentage points\nplot_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n\n\n\n\nNotice that so far, we have conducted counterfactual comparisons across deskmate and then simply averaged by gender_combination. In the counterfactual comparison step, we control for other variables in the model. But then when we average by gender_combination, we do not hold the other variables at any specific value – they simply naturally vary alongside gender_combination. Thus, when we compare the treatment effects between different gender_combinations, we compare them between groups that may also vary with respect to other variables, including those that are part of the model. One may think of these results as the unconditional association between gender_combination and the treatment effects. This is conceptually distinct from evaluating moderation by looking at model coefficients, since these coefficients are always conditional on all other predictors in the model. Thus, the default approach in the framework we champion differs from the default approach when interpreting coefficients, which warrants more discussion.\nShould we condition on the other predictors in the model when evaluating moderation? First, this depends on what we are interested in – if we just want to know whether a third variable is associated with some treatment effect, there is no need to condition on other variables (unless we are explicitly interested in the conditional association for some reason). However, if we are interested in a causal interaction, it may be necessary to condition on third variables to account for confounding (for a more extensive discussion of the causal status of interactions, see Rohrer & Arslan, 2021)). Second, in the latter scenario, whether to condition on a variable or not will depend on whether it is a confounder of the causal interaction (in which case we should condition on it) rather than, for example, a mediator.\nRecall that our model also contains GPA_difference. Pairs of students may vary in their GPA_difference, but these differences are plausibly causally downstream of gender_combination: For gender-mismatched dyads, we may observe larger differences in GPA because gender affects GPA (Gender combination → GPA difference → Effect of being seated next to each other). Thus, GPA_difference is a potential mediator of gender_combination and should not be conditioned on when evaluating whether gender_combination causally affects the effects of being seated next to each other. This vindicates our earlier analysis.\nIf we are interested in whether GPA_differences causally affect the effects of proximity, gender_combination is a confounder (GPA difference ← Gender combination → Effect of being seated next to each other). Thus, to evaluate the interaction between GPA_difference and Deskmate, we do want to condition gender_combination.\nSo, does the effect of being seated next to each other (deskmate) vary by how strong their academic achievement diverges (GPA_difference), controlling for the effects of gender match (gender_combination)? We can answer this question by conducting comparisons on new hypothetical datasets in which we set GPA_difference to particular values (e.g., ± 1 SD) but keep the other variables (including gender_match) as is.\n\n\n\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                GPA_difference\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.139\n                  0.1186\n                  0.0597\n                  0.182\n                \n                \n                  1.504\n                  0.0973\n                  0.0375\n                  0.157\n                \n        \n      \n    \n\n\n# the newdata argument is crucial here: we apply the model to new data which represents a counterfactual constrast\n# between a world in which everybody's GPA difference is set to 1 SD below the mean\n# and a world in which everybody's GPA difference is set to 1 SD above the mean.\n\n# Compare the two estimates\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"),\n                hypothesis = \"b1 = b2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b1=b2\n                  0.0212\n                  -0.0664\n                  0.116"
  },
  {
    "objectID": "examples/deskmates.html#overview",
    "href": "examples/deskmates.html#overview",
    "title": "Friends by Chance",
    "section": "",
    "text": "This document contains example 3, in which we analyze the effect of being seated next to each other on friendships, using Bayesian multilevel models. Data can be downloaded from the OSF but are also included in the downloadable replication package.\nEvery day experience—and previous research—suggests that being spatially close to others can result in friendships. But does spatial proximity also lead to friendships for people who are quite different from each other? We re-analyze data from a large field experiment conducted in 3rd to 8th grade classrooms in rural Hungary previously reported in Rohrer et al. (2021). Proximity was experimentally manipulated by randomizing each classrooms’ seating chart at the beginning of the school semester; thus, students randomly ended up next to each other (deskmate = 1) or not (deskmate = 0). At the end of the semester, students listed up to five best friends from their classroom, which allows us to determine which pairs of students had formed friendships (friendship = 1). Additionally, we know students’ gender and grade point average before the experiment (GPA), which allows us to investigate whether proximity also “works” for girls seated next to boys (who are quite unlikely to befriend each other at that age) or students with discrepant levels of academic achievement."
  },
  {
    "objectID": "examples/deskmates.html#read-and-clean-the-data",
    "href": "examples/deskmates.html#read-and-clean-the-data",
    "title": "Friends by Chance",
    "section": "",
    "text": "# Read the data\ndat &lt;- read.csv(here(\"data/deskmates.csv\"))\n\n# Arbitrary subset of 60 classes to make model-fitting less tedious\ndat &lt;- dat[dat$class_id %in% unique(dat$class_id)[1:60], ]\n\n# Keep complete cases\ndat &lt;- dat[complete.cases(dat),]\n\n\n# Label focal variables\ndat &lt;- transform(dat,\n    gender_combination = factor(girl_match, label = c(\"Boys\", \"Mixed\", \"Girls\")),\n    deskmate = factor(deskmate, label = c(\"Different desk\", \"Same desk\")))\n\n# Rename to match labels in manuscript\ndat &lt;- dat |&gt;\n  rename(GPA_average = mean_gpa,\n         GPA_difference = diff_gpa,\n         friendship = friend,\n         classroom = class_id,\n         student1 = s1,\n         student2 = s2)\n\n\n# Display the first few rows of data\nhead(dat)\n\n  any_single       deskmate sim_s1_s2 girl_s1 girl_s2 roma_s1 roma_s2 gpa_s1 gpa_s2 friendship friend_s1 friend_s2 girl_match roma_match GPA_average GPA_difference sim_s1_s2_std size size.1 gendermatch classroom school_id student1 student2 gender_combination\n1          0      Same desk 0.6500000       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.2570821   18     18           0     59379        32   731891   773914              Mixed\n2          0 Different desk 0.9566667       1       1       0       0    4.4    5.0          1         1         1          2          0         4.7            0.6     1.0919846   18     18           1     59379        32   731891   492505              Girls\n3          0 Different desk 0.6366667       1       0       0       0    4.4    4.0          0         0         0          1          0         4.2            0.4    -0.3157372   18     18           0     59379        32   731891   300996              Mixed\n4          0 Different desk 0.6233333       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.3743922   18     18           0     59379        32   731891   888954              Mixed\n5          0 Different desk 0.9833333       1       1       0       0    4.4    4.6          1         1         1          2          0         4.5            0.2     1.2092947   18     18           1     59379        32   731891   706146              Girls\n6          0 Different desk 0.6233333       1       0       0       0    4.4    5.0          0         0         0          1          0         4.7            0.6    -0.3743922   18     18           0     59379        32   731891   288809              Mixed"
  },
  {
    "objectID": "examples/deskmates.html#bayesian-multilevel-regression-model",
    "href": "examples/deskmates.html#bayesian-multilevel-regression-model",
    "title": "Friends by Chance",
    "section": "",
    "text": "Our model here ends up a bit more complex due to the nested structure of the data, and we use the brms package (Bürkner, 2018) which allows us to fit multilevel models in a highly flexible manner. Since we now fit a Bayesian model (relying on the default priors provided by the package), marginaleffects will return credible intervals rather than confidence intervals. Our unit of observation is pairs of students, which are nested within students and classrooms. For each pair, we know: * whether they are deskmates * their gender_combination (both boys, one girl and one boy, both girls) * their GPA_average (i.e., the average across both students) and their absolute GPA_difference (i.e., the discrepancy between both students) * whether they report a friendship at the end of the experiment or not\nHere, we fit and save the model to avoid refitting repeatedly.\n\nmod &lt;- brm(\n    friendship ~ deskmate + gender_combination + deskmate:gender_combination +\n        GPA_average + GPA_difference + GPA_average:deskmate + GPA_difference:deskmate +\n        (1 | classroom) + (1 | mm(student1, student2)),\n    family = bernoulli(link = \"logit\"),\n    data = dat,\n    seed = 12345,\n    cores = 4\n)\n\n# Save model fit\nsaveRDS(mod, file = here(\"fits/deskmates.rds\"))\n\n\n# Load the fitted model\nmod &lt;- readRDS(here(\"fits/deskmates.rds\"))\n\n# Look at the model coefficients\nparameters(mod)\n\n# Fixed Effects\n\nParameter                                |   Median |         95% CI |     pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)                              |    -1.62 | [-2.34, -0.94] |   100% | 1.000 | 1791.00\ndeskmateSamedesk                         |     0.22 | [-1.49,  1.96] | 60.02% | 1.001 | 2402.00\ngender_combinationMixed                  |    -3.54 | [-3.88, -3.23] |   100% | 1.002 | 2582.00\ngender_combinationGirls                  | 5.66e-03 | [-0.25,  0.25] | 51.75% | 1.001 | 2254.00\nGPA_average                              |     0.30 | [ 0.13,  0.49] | 99.95% | 1.001 | 1495.00\nGPA_difference                           |    -0.42 | [-0.56, -0.28] |   100% | 1.000 | 4267.00\ndeskmateSamedesk:gender_combinationMixed |     0.12 | [-0.86,  0.98] | 59.62% | 1.000 | 4483.00\ndeskmateSamedesk:gender_combinationGirls |     0.04 | [-0.69,  0.78] | 54.07% | 1.001 | 4559.00\ndeskmateSamedesk:GPA_average             |     0.18 | [-0.26,  0.62] | 79.67% | 1.000 | 2529.00\ndeskmateSamedesk:GPA_difference          |    -0.03 | [-0.60,  0.47] | 54.55% | 1.000 | 4436.00"
  },
  {
    "objectID": "examples/deskmates.html#interpretation-with-marginaleffects",
    "href": "examples/deskmates.html#interpretation-with-marginaleffects",
    "title": "Friends by Chance",
    "section": "",
    "text": "In principle, we could evaluate this model on the log-odds scale on which the coefficients are estimated.\n\navg_comparisons(mod, variables = \"deskmate\", type = \"link\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.958\n                  0.533\n                  1.35\n                \n        \n      \n    \n\n\n\nHowever, log-odds of friendship are not a particularly intuitive unit, and so instead we may want to switch to the scale of the outcome (friendships)—which is the default behavior of marginaleffects:\n\n\n\n\n# Predicted probabilities\navg_predictions(mod, by = \"deskmate\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                deskmate\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Different desk\n                  0.166\n                  0.158\n                  0.174\n                \n                \n                  Same desk\n                  0.270\n                  0.234\n                  0.311\n                \n        \n      \n    \n\n\n# Average Treatment Effect\nate &lt;- avg_comparisons(mod, variables = \"deskmate\")\nate\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.108\n                  0.0697\n                  0.15\n                \n        \n      \n    \n\n\n\nThis returns the average effect of the intervention in percentage points.\n\n\n\nOne may want to compare the effect of sitting next to each other to other interventions meant to foster friendships. For example, one staple of psychological research is the fast friends procedure in which two participants are paired up and then take turns answering questions that escalate in the degree of self-disclosure involved, from mild (“Would you like to be famous? In what way?”) to severe (“When did you last cry in front of another person?”). Echols and Ivanich (2021) implemented such a procedure in US middle school students and found that those who underwent the intervention in three sessions over three months were 10 percentage points more likely to become friends. This seems very close to the 11 percentage point effect we observed in our analysis.\nWould it be justified to conclude that the effects are practically the same?\nIn a Frequentist framework, this would be a use case for an equivalence test. Given our Bayesian model, we instead resort to the notion of a region of practical equivalence (ROPE; Kruschke, 2018; Makowski et al., 2019. First, we need to define a range around the 10 percentage points of the fast friends procedure for which we would consider the effects equivalent for practical purposes. Here, we decide that the effect ± a quarter of the effect is a sensible range, resulting in a ROPE of [0.075; 0.125].\nNow, we can calculate how likely it is that the effect of sitting next to each other falls into the ROPE of the fast friends procedure. For this, we additionally make use of the convenience provided by the posterior package:\n\nlibrary(posterior)\ndraws &lt;- get_draws(avg_comparisons(mod, variables = \"deskmate\"), \"rvar\")\n\nPr(draws$rvar &lt; 0.125) # Probability that below the upper bound\n\n[1] 0.79775\n\nPr(draws$rvar &lt; 0.075) # Probability that below the lower bound\n\n[1] 0.04175\n\n# In combination, from this we can conclude the probability that\n# the parameter lies within the bounds\nPr(draws$rvar &lt; 0.125) - Pr(draws$rvar &lt; 0.075)\n\n[1] 0.756\n\n\n\n\n\nHaving looked at the average effect of the intervention, we still do not yet know whether being seated next to each other also “works” for dissimilar students. Here, we will keep evaluating effects on the outcome scale, which we consider most intuitive. First, we can separately calculate average effects depending on gender_combination:\n\navg_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender_combination\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Boys\n                  0.1683\n                  0.07489\n                  0.2675\n                \n                \n                  Mixed\n                  0.0317\n                  0.00534\n                  0.0684\n                \n                \n                  Girls\n                  0.1940\n                  0.08685\n                  0.3018\n                \n        \n      \n    \n\n\n\n\n\nWe can easily compare all three average effects against each other in a pairwise manner:\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = ~pairwise)\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (Mixed) - (Boys)\n                  -0.1355\n                  -0.2384\n                  -0.0346\n                \n                \n                  (Girls) - (Boys)\n                  0.0273\n                  -0.1169\n                  0.1704\n                \n                \n                  (Girls) - (Mixed)\n                  0.1607\n                  0.0441\n                  0.2760\n                \n        \n      \n    \n\n\n\n\n\n\nWe may also want to compare gender-matched dyads (pairs of girls, pairs of boys) with gender-mismatched dyads (pairs of a girl and a boy), (b_Two girls + b_Two boys)/2 = b_One girl one boy. This can be achieved by using a different hypothesis argument (being mindful of the order in which the groups are listed in our output):\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = \"(b1 + b3)/2 = b2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (b1+b3)/2=b2\n                  0.149\n                  0.0667\n                  0.229\n                \n        \n      \n    \n\n\n\n\n\n\nWe can also visualize these different effects:\n\n# Plot the predicted probabilities\nplot_predictions(mod, by = c(\"gender_combination\", \"deskmate\"))\n\n\n\n# Plot the effects of the intervention in percentage points\nplot_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n\n\n\n\nNotice that so far, we have conducted counterfactual comparisons across deskmate and then simply averaged by gender_combination. In the counterfactual comparison step, we control for other variables in the model. But then when we average by gender_combination, we do not hold the other variables at any specific value – they simply naturally vary alongside gender_combination. Thus, when we compare the treatment effects between different gender_combinations, we compare them between groups that may also vary with respect to other variables, including those that are part of the model. One may think of these results as the unconditional association between gender_combination and the treatment effects. This is conceptually distinct from evaluating moderation by looking at model coefficients, since these coefficients are always conditional on all other predictors in the model. Thus, the default approach in the framework we champion differs from the default approach when interpreting coefficients, which warrants more discussion.\nShould we condition on the other predictors in the model when evaluating moderation? First, this depends on what we are interested in – if we just want to know whether a third variable is associated with some treatment effect, there is no need to condition on other variables (unless we are explicitly interested in the conditional association for some reason). However, if we are interested in a causal interaction, it may be necessary to condition on third variables to account for confounding (for a more extensive discussion of the causal status of interactions, see Rohrer & Arslan, 2021)). Second, in the latter scenario, whether to condition on a variable or not will depend on whether it is a confounder of the causal interaction (in which case we should condition on it) rather than, for example, a mediator.\nRecall that our model also contains GPA_difference. Pairs of students may vary in their GPA_difference, but these differences are plausibly causally downstream of gender_combination: For gender-mismatched dyads, we may observe larger differences in GPA because gender affects GPA (Gender combination → GPA difference → Effect of being seated next to each other). Thus, GPA_difference is a potential mediator of gender_combination and should not be conditioned on when evaluating whether gender_combination causally affects the effects of being seated next to each other. This vindicates our earlier analysis.\nIf we are interested in whether GPA_differences causally affect the effects of proximity, gender_combination is a confounder (GPA difference ← Gender combination → Effect of being seated next to each other). Thus, to evaluate the interaction between GPA_difference and Deskmate, we do want to condition gender_combination.\nSo, does the effect of being seated next to each other (deskmate) vary by how strong their academic achievement diverges (GPA_difference), controlling for the effects of gender match (gender_combination)? We can answer this question by conducting comparisons on new hypothetical datasets in which we set GPA_difference to particular values (e.g., ± 1 SD) but keep the other variables (including gender_match) as is.\n\n\n\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"))\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                GPA_difference\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.139\n                  0.1186\n                  0.0597\n                  0.182\n                \n                \n                  1.504\n                  0.0973\n                  0.0375\n                  0.157\n                \n        \n      \n    \n\n\n# the newdata argument is crucial here: we apply the model to new data which represents a counterfactual constrast\n# between a world in which everybody's GPA difference is set to 1 SD below the mean\n# and a world in which everybody's GPA difference is set to 1 SD above the mean.\n\n# Compare the two estimates\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"),\n                hypothesis = \"b1 = b2\")\n\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b1=b2\n                  0.0212\n                  -0.0664\n                  0.116"
  }
]