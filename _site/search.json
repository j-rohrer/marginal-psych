[
  {
    "objectID": "examples/relationship.html",
    "href": "examples/relationship.html",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "Before we start, let’s execute a helper script that loads the necessary dependencies.\n\nsource(here::here(\"scripts/load.R\"))\n\n\n\nThis document contains example 2, in which we analyze the association between relationship status and the importance people assign to friends. Data can be retrieved from the OSF but is also included in the downloadable replication package.\nIt’s a common complaint that people who enter a relationship start to neglect their friends. Here, we are going to use this to motivate an associational research question: Do people in romantic relationships, on average, assign less importance to their friends? To address this question, we analyze data that were collected in the context of a diary study on satisfaction with various aspects of life (Rohrer et al., 2024). In this study, 482 people reported whether they were in a romantic relationship of any kind (partner) and also the extent to which they considered their friendships important (friendship_importance) on a scale from 1 (not important at all) to 5 (very important).\n\n\n\n\n# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\n# Look at the variables\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\n# Rename some variables\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)\n\n\n\n\nThis code produces Figure 4: Predicted importance of friends by age in three different simple linear models that only include age as a predictor, including (1) age as a linear predictor or (2) age as a categorical predictor, or (3) age splines (B-splines with four degrees of freedom).\nRespondents’ age varies from 18 to 59 years. How do we best include this variable in our analysis? If we simply include it as a linear predictor, we assume that friendship_importance changes with age in a linear manner. If, instead, we include it as a categorical predictor (treating each year of age as its own category), we do not impose any assumptions about the functional form—but for some years, we only have few observations, resulting in a trajectory that jumps around a lot, with wide confidence intervals.\nSo, we may prefer a solution that lies somewhere between these two options. Contenders may be using coarser age categories or using polynomials. A third alternative is splines. These result in flexible, locally smooth trajectories. Unlike polynomials, splines enforce no global functional forms; unlike age categories, splines do not result in abrupt jumps in the trajectory.\n\n# Linear association\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\n# Each year of age as its own category\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n# Smoothed with splines\nage_smooth &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\n\n# Extract predictions from each of the three models\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\npred_smooth &lt;- avg_predictions(age_smooth, by = \"age\")\n\n# The following code generates a grid that expands the categorical predictions\n# so that we can plot them as a step function including a step ribbon\n# (This is just to get the visuals of Figure 4 right and nothing substantive)\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# Color scheme for Figure 4\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#D55E00\"\ncol_smooth &lt;- \"#CC79A7\"\n\n# Generate the plot\nggplot() +\n  # categorical\n  geom_line(data = pred_cat_expanded, aes(x = age, y = estimate, group = floor(age)), color = col_cat) +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  # smoothed\n  geom_line(data = pred_smooth, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_smooth, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_smooth) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Importance of friends (95% CI)\")\n\n\n\n\n\n\n\n# ggsave(here(\"plots/age.png\"), width = 4, height = 3)\n\n\n\n\n\nmod &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\n\n# Predictions for a 20 year old single\npredictions(mod, newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  4.66\n                  0.243\n                  19.2\n                  &lt;0.001\n                  269.4\n                  4.18\n                  5.13\n                \n        \n      \n    \n\n\n# Slope of age for a 20 year old single\nslopes(mod, variables = \"age\", newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  0.00329\n                  0.104\n                  0.0318\n                  0.975\n                  0.0\n                  -0.2\n                  0.207\n                \n        \n      \n    \n\n\n# Predictions for everybody in the data\npredictions(mod)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  4.46\n                  0.1485\n                  30.0\n                  &lt;0.001\n                  654.5\n                  4.17\n                  4.75\n                \n                \n                  4.58\n                  0.1362\n                  33.6\n                  &lt;0.001\n                  819.9\n                  4.31\n                  4.85\n                \n                \n                  4.45\n                  0.1131\n                  39.3\n                  &lt;0.001\n                  Inf\n                  4.23\n                  4.67\n                \n                \n                  4.41\n                  0.0994\n                  44.4\n                  &lt;0.001\n                  Inf\n                  4.22\n                  4.61\n                \n                \n                  4.52\n                  0.1447\n                  31.2\n                  &lt;0.001\n                  709.7\n                  4.24\n                  4.81\n                \n                \n                  472 rows omitted\n                  \n                  \n                  \n                  \n                  \n                  \n                \n                \n                  4.66\n                  0.1341\n                  34.8\n                  &lt;0.001\n                  876.7\n                  4.40\n                  4.92\n                \n                \n                  4.63\n                  0.1872\n                  24.7\n                  &lt;0.001\n                  446.4\n                  4.26\n                  5.00\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.67\n                  0.1104\n                  42.2\n                  &lt;0.001\n                  Inf\n                  4.45\n                  4.88\n                \n        \n      \n    \n\n\n# Main result: Difference in friendship importance with and without partner, holding constant age and gender\navg_comparisons(mod, variables = \"partner\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.072\n                  0.0804\n                  -0.896\n                  0.37\n                  1.4\n                  -0.23\n                  0.0856\n                \n        \n      \n    \n\n\n# Generate separate estimates of the difference, by gender\navg_comparisons(mod, variables = \"partner\", by = \"gender\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  female\n                  -0.0738\n                  0.0946\n                  -0.780\n                  0.436\n                  1.2\n                  -0.259\n                  0.112\n                \n                \n                  male\n                  -0.0675\n                  0.1529\n                  -0.442\n                  0.659\n                  0.6\n                  -0.367\n                  0.232\n                \n        \n      \n    \n\n\n# Compare these gender-specific estimates\navg_comparisons(mod, variables = \"partner\", by = \"gender\", hypothesis = \"b2 - b1 = 0\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b2-b1=0\n                  0.00625\n                  0.18\n                  0.0348\n                  0.972\n                  0.0\n                  -0.346\n                  0.359\n                \n        \n      \n    \n\n\n# For the sake of completeness, we may also generate age-specific estimates\n\n# Note that the wiggliness of this line will depend on how we modeled age\ncomp &lt;- avg_comparisons(mod, variables = \"partner\", by = \"age\")\n\nggplot(comp, aes(x = age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() +\n  geom_ribbon(alpha = .1) +\n  geom_hline(yintercept = 0) +\n  ylab(\"Association partner and friendship importance\")\n\n\n\n\n\n\n\n\n\n\n\nSo far, we have simply conducted linear regressions, but that may be considered suspect given the nature of the outcome: it’s just a five-point response scale ranging from not important at all to very important. And, in fact, barely anybody used the lower response options—more than 40% picked the highest response option. This results in a distribution for which the assumptions of linear regression may be considered heroic.\nSo, let’s run an ordinal regression to see whether conclusions change. Here, we are going to fit a cumulative ordinal model with a probit link using the ordinal package (Christensen, 2023) using the clm() function. In essence, this approach assumes a continuous, normally distributed standardized latent variable (“true” friendship_importance) which is translated into the ordinal response variable following thresholds that are estimated from the data; for example, people who score -3 or less on the standardized latent variable may report that their friends are not important at all, people who score more than that but below -2.7 may pick the second lowest response option, and so on (see Bürkner & Vuorre, 2019 for a proper introduction to these models).\nThe rest of the model specification remains unchanged:\n\n# Outcome as factor variable which is what clm() expects\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\n\n# Fit the model\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\n\n# Evaluate central comparison in terms of change in response probabilities\navg_comparisons(mod_ord, variables = \"partner\") \n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00170\n                  0.00436\n                  0.390\n                  0.6963\n                  0.5\n                  -0.00685\n                  0.0103\n                \n                \n                  2\n                  0.00353\n                  0.00555\n                  0.636\n                  0.5247\n                  0.9\n                  -0.00735\n                  0.0144\n                \n                \n                  3\n                  0.02129\n                  0.01963\n                  1.085\n                  0.2780\n                  1.8\n                  -0.01718\n                  0.0598\n                \n                \n                  4\n                  0.02896\n                  0.01641\n                  1.765\n                  0.0776\n                  3.7\n                  -0.00320\n                  0.0611\n                \n                \n                  5\n                  -0.05549\n                  0.03847\n                  -1.442\n                  0.1492\n                  2.7\n                  -0.13089\n                  0.0199\n                \n        \n      \n    \n\n\n# these results seem reasonable\n\nIt’s hard to compare these results to the results from the linear model. If one really wants to generate an estimate that is as comparable as possible, one can assign integer values from 1 to 5 to the response categories and thus estimate what the ordinal model implies for the average:\n\n# Take the response probabilities and attach the integers 1:5 to the consecutive\n# response categories to arrive at the same metric as the linear model\navg_comparisons(mod_ord, variables = \"partner\", hypothesis = ~ I(sum(x * 1:5)))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.089\n                  0.0784\n                  -1.13\n                  0.257\n                  2.0\n                  -0.243\n                  0.0647\n                \n        \n      \n    \n\n\n\nThis makes use of the hypothesis argument, which allows researchers to conduct tests on arbitrary transformations of the quantities of interest. Here, we take each row of the standard output (the change in the probability of a given response option with partner), multiply it with the corresponding integer value (1 to 5), and sum everything up.\nWhile this ensures that we ask the ordinal model precisely the same answer that we asked the linear model, it may appear a bit inconsistent given that we usually use ordinal models precisely because we don’t want to assign integer values to the response categories – although note that here we do it only in the very last step, so the model itself does not assume that the distances between adjacent response categories are the same.\nMuch more in the spirit of the ordinal model, we can also compute the effect on the underlying continuous variable that the model assumes.\n\n# Evaluate effect on the underlying latent variable\navg_comparisons(mod_ord, variables = \"partner\", type = \"linear.predictor\") \n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  2\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  3\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  4\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  5\n                  0.147\n                  0.107\n                  1.37\n                  0.170\n                  2.6\n                  -0.0627\n                  0.356\n                \n        \n      \n    \n\n\n# NOTE: This currently returns the wrong sign. Bug to be fixed\n\n\n\n\nLet’s vary model complexity to see what happens to our target quantity, the association between having a partner and friendship importance (holding constant age and gender).\n\n\n\n# No age in the model\nmod_age_0 &lt;- lm(friendship_importance ~ gender + partner +\n            partner:gender, data = dat)\ncomp_age_0 &lt;- avg_comparisons(mod_age_0, variables = \"partner\")\ncomp_age_0$model &lt;- \"Not included\"\n\n# Linear age in the model\nmod_age_1 &lt;- lm(friendship_importance ~ age + gender + partner +\n            age:gender + partner:gender + age:partner, data = dat)\ncomp_age_1 &lt;- avg_comparisons(mod_age_1, variables = \"partner\")\ncomp_age_1$model &lt;- \"Linear\"\n\n# Quadratic age on top\nmod_age_2 &lt;- lm(friendship_importance ~ poly(age, 2) + gender + partner +\n            poly(age, 2):gender + partner:gender + poly(age, 2):partner, data = dat)\ncomp_age_2 &lt;- avg_comparisons(mod_age_2, variables = \"partner\")\ncomp_age_2$model &lt;- \"Quadratic\"\n\n# Cubic age on top\nmod_age_3 &lt;- lm(friendship_importance ~ poly(age, 3) + gender + partner +\n            poly(age, 3):gender + partner:gender + poly(age, 3):partner, data = dat)\ncomp_age_3 &lt;- avg_comparisons(mod_age_3, variables = \"partner\")\ncomp_age_3$model &lt;- \"Cubic\"\n\n# Quartic age on tope\nmod_age_4 &lt;- lm(friendship_importance ~ poly(age, 4) + gender + partner +\n            poly(age, 4):gender + partner:gender + poly(age, 4):partner, data = dat)\ncomp_age_4 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_4$model &lt;- \"Quartic\"\n\n# Quintic age on tope\nmod_age_5 &lt;- lm(friendship_importance ~ poly(age, 5) + gender + partner +\n            poly(age, 5):gender + partner:gender + poly(age, 5):partner, data = dat)\ncomp_age_5 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_5$model &lt;- \"Quintic\"\n\n\n# Age with splines, df = 3\nmod_age_6 &lt;- lm(friendship_importance ~ bs(age, df = 3) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_6 &lt;- avg_comparisons(mod_age_6, variables = \"partner\")\ncomp_age_6$model &lt;- \"Splines, df = 3\"\n\n\n# Splines, df = 4 (same as model reported in main text)\nmod_age_7 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_7 &lt;- avg_comparisons(mod_age_7, variables = \"partner\")\ncomp_age_7$model &lt;- \"Splines, df = 4\"\n\n# Splines, df = 5\nmod_age_8 &lt;- lm(friendship_importance ~ bs(age, df = 5) + gender + partner +\n                  bs(age, df = 5):gender + partner:gender + bs(age, df = 5):partner, data = dat)\ncomp_age_8 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_8$model &lt;- \"Splines, df = 5\"\n\n# Splines, df = 6\nmod_age_9 &lt;- lm(friendship_importance ~ bs(age, df = 6) + gender + partner +\n                  bs(age, df = 6):gender + partner:gender + bs(age, df = 6):partner, data = dat)\ncomp_age_9 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_9$model &lt;- \"Splines, df = 6\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_age &lt;- rbind(data.frame(comp_age_0), data.frame(comp_age_1), \n                  data.frame(comp_age_2), data.frame(comp_age_3),\n                  data.frame(comp_age_4), data.frame(comp_age_5),\n                  data.frame(comp_age_6), data.frame(comp_age_7),\n                  data.frame(comp_age_8), data.frame(comp_age_9))\n\n# Numbering (for plotting purposes)\ncomp_age$no &lt;- 1:nrow(comp_age)\n\n# Generate the plot\nggplot(comp_age, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_age$no, labels = comp_age$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Way that age is included in model\") +\n  ylab(\"Target quantity (95% CI)\")\n\n\n\n\n\n\n\n# ggsave(here(\"plots/age_robustness.png\"), width = 4, height = 3)\n\n\n\n\nLet’s see whether the inclusion of interactions makes a difference\n\n# No interactions\nmod_interact_1 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner, data = dat)\ncomp_interact_1 &lt;- avg_comparisons(mod_interact_1, variables = \"partner\")\ncomp_interact_1$model &lt;- \"No interaction\"\n\n# Add two-way interactions (same as model reported in main text)\nmod_interact_2 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                       bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\ncomp_interact_2 &lt;- avg_comparisons(mod_interact_2, variables = \"partner\")\ncomp_interact_2$model &lt;- \"Two-way interactions\"\n\n# Add three-way interaction\nmod_interact_3 &lt;- lm(friendship_importance ~ bs(age, df = 4)*gender*partner, data = dat)\ncomp_interact_3 &lt;- avg_comparisons(mod_interact_3, variables = \"partner\")\ncomp_interact_3$model &lt;- \"Three-way interaction\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_interact &lt;- rbind(data.frame(comp_interact_1), data.frame(comp_interact_2), data.frame(comp_interact_3))\n\n# Numbering (for plotting purposes)\ncomp_interact$no &lt;- 1:nrow(comp_interact)\n\n# Plot the results\nggplot(comp_interact, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_interact$no, labels = comp_interact$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Interactions included\") +\n  ylab(\"Estimated difference in friendship importance\")"
  },
  {
    "objectID": "examples/relationship.html#overview",
    "href": "examples/relationship.html#overview",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "This document contains example 2, in which we analyze the association between relationship status and the importance people assign to friends. Data can be retrieved from the OSF but is also included in the downloadable replication package.\nIt’s a common complaint that people who enter a relationship start to neglect their friends. Here, we are going to use this to motivate an associational research question: Do people in romantic relationships, on average, assign less importance to their friends? To address this question, we analyze data that were collected in the context of a diary study on satisfaction with various aspects of life (Rohrer et al., 2024). In this study, 482 people reported whether they were in a romantic relationship of any kind (partner) and also the extent to which they considered their friendships important (friendship_importance) on a scale from 1 (not important at all) to 5 (very important)."
  },
  {
    "objectID": "examples/relationship.html#read-and-clean-the-data",
    "href": "examples/relationship.html#read-and-clean-the-data",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\n# Look at the variables\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\n# Rename some variables\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)"
  },
  {
    "objectID": "examples/relationship.html#visualize-associations-with-age",
    "href": "examples/relationship.html#visualize-associations-with-age",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "This code produces Figure 4: Predicted importance of friends by age in three different simple linear models that only include age as a predictor, including (1) age as a linear predictor or (2) age as a categorical predictor, or (3) age splines (B-splines with four degrees of freedom).\nRespondents’ age varies from 18 to 59 years. How do we best include this variable in our analysis? If we simply include it as a linear predictor, we assume that friendship_importance changes with age in a linear manner. If, instead, we include it as a categorical predictor (treating each year of age as its own category), we do not impose any assumptions about the functional form—but for some years, we only have few observations, resulting in a trajectory that jumps around a lot, with wide confidence intervals.\nSo, we may prefer a solution that lies somewhere between these two options. Contenders may be using coarser age categories or using polynomials. A third alternative is splines. These result in flexible, locally smooth trajectories. Unlike polynomials, splines enforce no global functional forms; unlike age categories, splines do not result in abrupt jumps in the trajectory.\n\n# Linear association\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\n# Each year of age as its own category\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n# Smoothed with splines\nage_smooth &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\n\n# Extract predictions from each of the three models\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\npred_smooth &lt;- avg_predictions(age_smooth, by = \"age\")\n\n# The following code generates a grid that expands the categorical predictions\n# so that we can plot them as a step function including a step ribbon\n# (This is just to get the visuals of Figure 4 right and nothing substantive)\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# Color scheme for Figure 4\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#D55E00\"\ncol_smooth &lt;- \"#CC79A7\"\n\n# Generate the plot\nggplot() +\n  # categorical\n  geom_line(data = pred_cat_expanded, aes(x = age, y = estimate, group = floor(age)), color = col_cat) +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  # smoothed\n  geom_line(data = pred_smooth, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_smooth, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_smooth) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Importance of friends (95% CI)\")\n\n\n\n\n\n\n\n# ggsave(here(\"plots/age.png\"), width = 4, height = 3)"
  },
  {
    "objectID": "examples/relationship.html#model-fitting-and-interpretation-with-marginaleffects",
    "href": "examples/relationship.html#model-fitting-and-interpretation-with-marginaleffects",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "mod &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\n\n# Predictions for a 20 year old single\npredictions(mod, newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  4.66\n                  0.243\n                  19.2\n                  &lt;0.001\n                  269.4\n                  4.18\n                  5.13\n                \n        \n      \n    \n\n\n# Slope of age for a 20 year old single\nslopes(mod, variables = \"age\", newdata = datagrid(age = 20, gender = \"male\", partner = 0))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                age\n                gender\n                partner\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  20\n                  male\n                  0\n                  0.00329\n                  0.104\n                  0.0318\n                  0.975\n                  0.0\n                  -0.2\n                  0.207\n                \n        \n      \n    \n\n\n# Predictions for everybody in the data\npredictions(mod)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  4.46\n                  0.1485\n                  30.0\n                  &lt;0.001\n                  654.5\n                  4.17\n                  4.75\n                \n                \n                  4.58\n                  0.1362\n                  33.6\n                  &lt;0.001\n                  819.9\n                  4.31\n                  4.85\n                \n                \n                  4.45\n                  0.1131\n                  39.3\n                  &lt;0.001\n                  Inf\n                  4.23\n                  4.67\n                \n                \n                  4.41\n                  0.0994\n                  44.4\n                  &lt;0.001\n                  Inf\n                  4.22\n                  4.61\n                \n                \n                  4.52\n                  0.1447\n                  31.2\n                  &lt;0.001\n                  709.7\n                  4.24\n                  4.81\n                \n                \n                  472 rows omitted\n                  \n                  \n                  \n                  \n                  \n                  \n                \n                \n                  4.66\n                  0.1341\n                  34.8\n                  &lt;0.001\n                  876.7\n                  4.40\n                  4.92\n                \n                \n                  4.63\n                  0.1872\n                  24.7\n                  &lt;0.001\n                  446.4\n                  4.26\n                  5.00\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.46\n                  0.1480\n                  30.1\n                  &lt;0.001\n                  659.1\n                  4.17\n                  4.75\n                \n                \n                  4.67\n                  0.1104\n                  42.2\n                  &lt;0.001\n                  Inf\n                  4.45\n                  4.88\n                \n        \n      \n    \n\n\n# Main result: Difference in friendship importance with and without partner, holding constant age and gender\navg_comparisons(mod, variables = \"partner\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.072\n                  0.0804\n                  -0.896\n                  0.37\n                  1.4\n                  -0.23\n                  0.0856\n                \n        \n      \n    \n\n\n# Generate separate estimates of the difference, by gender\navg_comparisons(mod, variables = \"partner\", by = \"gender\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  female\n                  -0.0738\n                  0.0946\n                  -0.780\n                  0.436\n                  1.2\n                  -0.259\n                  0.112\n                \n                \n                  male\n                  -0.0675\n                  0.1529\n                  -0.442\n                  0.659\n                  0.6\n                  -0.367\n                  0.232\n                \n        \n      \n    \n\n\n# Compare these gender-specific estimates\navg_comparisons(mod, variables = \"partner\", by = \"gender\", hypothesis = \"b2 - b1 = 0\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b2-b1=0\n                  0.00625\n                  0.18\n                  0.0348\n                  0.972\n                  0.0\n                  -0.346\n                  0.359\n                \n        \n      \n    \n\n\n# For the sake of completeness, we may also generate age-specific estimates\n\n# Note that the wiggliness of this line will depend on how we modeled age\ncomp &lt;- avg_comparisons(mod, variables = \"partner\", by = \"age\")\n\nggplot(comp, aes(x = age, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_line() +\n  geom_ribbon(alpha = .1) +\n  geom_hline(yintercept = 0) +\n  ylab(\"Association partner and friendship importance\")"
  },
  {
    "objectID": "examples/relationship.html#ordinal-robustness-check",
    "href": "examples/relationship.html#ordinal-robustness-check",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "So far, we have simply conducted linear regressions, but that may be considered suspect given the nature of the outcome: it’s just a five-point response scale ranging from not important at all to very important. And, in fact, barely anybody used the lower response options—more than 40% picked the highest response option. This results in a distribution for which the assumptions of linear regression may be considered heroic.\nSo, let’s run an ordinal regression to see whether conclusions change. Here, we are going to fit a cumulative ordinal model with a probit link using the ordinal package (Christensen, 2023) using the clm() function. In essence, this approach assumes a continuous, normally distributed standardized latent variable (“true” friendship_importance) which is translated into the ordinal response variable following thresholds that are estimated from the data; for example, people who score -3 or less on the standardized latent variable may report that their friends are not important at all, people who score more than that but below -2.7 may pick the second lowest response option, and so on (see Bürkner & Vuorre, 2019 for a proper introduction to these models).\nThe rest of the model specification remains unchanged:\n\n# Outcome as factor variable which is what clm() expects\ndat$friendship_importance_factor &lt;- factor(dat$friendship_importance, ordered = TRUE)\n\n# Fit the model\nmod_ord &lt;- clm(friendship_importance_factor ~ bs(age, df = 4) + gender + partner +\n            bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, \n            data = dat,\n            link = \"probit\")\n\n# Evaluate central comparison in terms of change in response probabilities\navg_comparisons(mod_ord, variables = \"partner\") \n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.00170\n                  0.00436\n                  0.390\n                  0.6963\n                  0.5\n                  -0.00685\n                  0.0103\n                \n                \n                  2\n                  0.00353\n                  0.00555\n                  0.636\n                  0.5247\n                  0.9\n                  -0.00735\n                  0.0144\n                \n                \n                  3\n                  0.02129\n                  0.01963\n                  1.085\n                  0.2780\n                  1.8\n                  -0.01718\n                  0.0598\n                \n                \n                  4\n                  0.02896\n                  0.01641\n                  1.765\n                  0.0776\n                  3.7\n                  -0.00320\n                  0.0611\n                \n                \n                  5\n                  -0.05549\n                  0.03847\n                  -1.442\n                  0.1492\n                  2.7\n                  -0.13089\n                  0.0199\n                \n        \n      \n    \n\n\n# these results seem reasonable\n\nIt’s hard to compare these results to the results from the linear model. If one really wants to generate an estimate that is as comparable as possible, one can assign integer values from 1 to 5 to the response categories and thus estimate what the ordinal model implies for the average:\n\n# Take the response probabilities and attach the integers 1:5 to the consecutive\n# response categories to arrive at the same metric as the linear model\navg_comparisons(mod_ord, variables = \"partner\", hypothesis = ~ I(sum(x * 1:5)))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  -0.089\n                  0.0784\n                  -1.13\n                  0.257\n                  2.0\n                  -0.243\n                  0.0647\n                \n        \n      \n    \n\n\n\nThis makes use of the hypothesis argument, which allows researchers to conduct tests on arbitrary transformations of the quantities of interest. Here, we take each row of the standard output (the change in the probability of a given response option with partner), multiply it with the corresponding integer value (1 to 5), and sum everything up.\nWhile this ensures that we ask the ordinal model precisely the same answer that we asked the linear model, it may appear a bit inconsistent given that we usually use ordinal models precisely because we don’t want to assign integer values to the response categories – although note that here we do it only in the very last step, so the model itself does not assume that the distances between adjacent response categories are the same.\nMuch more in the spirit of the ordinal model, we can also compute the effect on the underlying continuous variable that the model assumes.\n\n# Evaluate effect on the underlying latent variable\navg_comparisons(mod_ord, variables = \"partner\", type = \"linear.predictor\") \n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Group\n                Estimate\n                Std. Error\n                z\n                Pr(&gt;|z|)\n                S\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  1\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  2\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  3\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  4\n                  0.147\n                  0.107\n                  1.38\n                  0.169\n                  2.6\n                  -0.0623\n                  0.355\n                \n                \n                  5\n                  0.147\n                  0.107\n                  1.37\n                  0.170\n                  2.6\n                  -0.0627\n                  0.356\n                \n        \n      \n    \n\n\n# NOTE: This currently returns the wrong sign. Bug to be fixed"
  },
  {
    "objectID": "examples/relationship.html#varying-the-complexity-of-the-linear-model-to-see-what-happens",
    "href": "examples/relationship.html#varying-the-complexity-of-the-linear-model-to-see-what-happens",
    "title": "Relationship Status and the Importance of Friends",
    "section": "",
    "text": "Let’s vary model complexity to see what happens to our target quantity, the association between having a partner and friendship importance (holding constant age and gender).\n\n\n\n# No age in the model\nmod_age_0 &lt;- lm(friendship_importance ~ gender + partner +\n            partner:gender, data = dat)\ncomp_age_0 &lt;- avg_comparisons(mod_age_0, variables = \"partner\")\ncomp_age_0$model &lt;- \"Not included\"\n\n# Linear age in the model\nmod_age_1 &lt;- lm(friendship_importance ~ age + gender + partner +\n            age:gender + partner:gender + age:partner, data = dat)\ncomp_age_1 &lt;- avg_comparisons(mod_age_1, variables = \"partner\")\ncomp_age_1$model &lt;- \"Linear\"\n\n# Quadratic age on top\nmod_age_2 &lt;- lm(friendship_importance ~ poly(age, 2) + gender + partner +\n            poly(age, 2):gender + partner:gender + poly(age, 2):partner, data = dat)\ncomp_age_2 &lt;- avg_comparisons(mod_age_2, variables = \"partner\")\ncomp_age_2$model &lt;- \"Quadratic\"\n\n# Cubic age on top\nmod_age_3 &lt;- lm(friendship_importance ~ poly(age, 3) + gender + partner +\n            poly(age, 3):gender + partner:gender + poly(age, 3):partner, data = dat)\ncomp_age_3 &lt;- avg_comparisons(mod_age_3, variables = \"partner\")\ncomp_age_3$model &lt;- \"Cubic\"\n\n# Quartic age on tope\nmod_age_4 &lt;- lm(friendship_importance ~ poly(age, 4) + gender + partner +\n            poly(age, 4):gender + partner:gender + poly(age, 4):partner, data = dat)\ncomp_age_4 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_4$model &lt;- \"Quartic\"\n\n# Quintic age on tope\nmod_age_5 &lt;- lm(friendship_importance ~ poly(age, 5) + gender + partner +\n            poly(age, 5):gender + partner:gender + poly(age, 5):partner, data = dat)\ncomp_age_5 &lt;- avg_comparisons(mod_age_4, variables = \"partner\")\ncomp_age_5$model &lt;- \"Quintic\"\n\n\n# Age with splines, df = 3\nmod_age_6 &lt;- lm(friendship_importance ~ bs(age, df = 3) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_6 &lt;- avg_comparisons(mod_age_6, variables = \"partner\")\ncomp_age_6$model &lt;- \"Splines, df = 3\"\n\n\n# Splines, df = 4 (same as model reported in main text)\nmod_age_7 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                  bs(age, df = 3):gender + partner:gender + bs(age, df = 3):partner, data = dat)\ncomp_age_7 &lt;- avg_comparisons(mod_age_7, variables = \"partner\")\ncomp_age_7$model &lt;- \"Splines, df = 4\"\n\n# Splines, df = 5\nmod_age_8 &lt;- lm(friendship_importance ~ bs(age, df = 5) + gender + partner +\n                  bs(age, df = 5):gender + partner:gender + bs(age, df = 5):partner, data = dat)\ncomp_age_8 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_8$model &lt;- \"Splines, df = 5\"\n\n# Splines, df = 6\nmod_age_9 &lt;- lm(friendship_importance ~ bs(age, df = 6) + gender + partner +\n                  bs(age, df = 6):gender + partner:gender + bs(age, df = 6):partner, data = dat)\ncomp_age_9 &lt;- avg_comparisons(mod_age_8, variables = \"partner\")\ncomp_age_9$model &lt;- \"Splines, df = 6\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_age &lt;- rbind(data.frame(comp_age_0), data.frame(comp_age_1), \n                  data.frame(comp_age_2), data.frame(comp_age_3),\n                  data.frame(comp_age_4), data.frame(comp_age_5),\n                  data.frame(comp_age_6), data.frame(comp_age_7),\n                  data.frame(comp_age_8), data.frame(comp_age_9))\n\n# Numbering (for plotting purposes)\ncomp_age$no &lt;- 1:nrow(comp_age)\n\n# Generate the plot\nggplot(comp_age, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_age$no, labels = comp_age$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Way that age is included in model\") +\n  ylab(\"Target quantity (95% CI)\")\n\n\n\n\n\n\n\n# ggsave(here(\"plots/age_robustness.png\"), width = 4, height = 3)\n\n\n\n\nLet’s see whether the inclusion of interactions makes a difference\n\n# No interactions\nmod_interact_1 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner, data = dat)\ncomp_interact_1 &lt;- avg_comparisons(mod_interact_1, variables = \"partner\")\ncomp_interact_1$model &lt;- \"No interaction\"\n\n# Add two-way interactions (same as model reported in main text)\nmod_interact_2 &lt;- lm(friendship_importance ~ bs(age, df = 4) + gender + partner +\n                       bs(age, df = 4):gender + partner:gender + bs(age, df = 4):partner, data = dat)\ncomp_interact_2 &lt;- avg_comparisons(mod_interact_2, variables = \"partner\")\ncomp_interact_2$model &lt;- \"Two-way interactions\"\n\n# Add three-way interaction\nmod_interact_3 &lt;- lm(friendship_importance ~ bs(age, df = 4)*gender*partner, data = dat)\ncomp_interact_3 &lt;- avg_comparisons(mod_interact_3, variables = \"partner\")\ncomp_interact_3$model &lt;- \"Three-way interaction\"\n\n# Put all the comparisons from the different model\n# into one dataframe for plotting purposes\ncomp_interact &lt;- rbind(data.frame(comp_interact_1), data.frame(comp_interact_2), data.frame(comp_interact_3))\n\n# Numbering (for plotting purposes)\ncomp_interact$no &lt;- 1:nrow(comp_interact)\n\n# Plot the results\nggplot(comp_interact, aes(x = no, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_point() +\n  geom_errorbar() +\n  geom_hline(yintercept = 0) +\n  coord_cartesian(ylim = c(-0.4, 0.4)) +\n  scale_x_continuous(breaks = comp_interact$no, labels = comp_interact$model) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +\n  xlab(\"Interactions included\") +\n  ylab(\"Estimated difference in friendship importance\")"
  },
  {
    "objectID": "examples/age.html",
    "href": "examples/age.html",
    "title": "Age trajectory",
    "section": "",
    "text": "source(here::here(\"scripts/load.R\"))\n\n# Read the data\ndat &lt;- read.csv(here(\"data/start.csv\"))\n\n# Restrict to range that we can model well\ndat &lt;- dat[dat$sex != 3, ] # exclude 5 people who reported a gender distinct from male/female\ndat &lt;- dat[dat$age &lt; 60, ] # exclude people over the age of 60\n\n# Limit to complete cases\ndat &lt;- dat[complete.cases(dat[, c(\"age\", \"sex\", \"partner_any\", \"IMP_friends_Start\")]),]\n\ndat$sex &lt;- as.factor(dat$sex)\n\ntable(dat$age)\n\n\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 \n15 28 29 20 15 18 13 16 16 12  4 14  8  3  6  6 13 18 11  9 12 13  6  7 11  6  5  3 11 12 12  7 15 13 14  7  5 11  9 11 10  8 \n\ntable(dat$sex)\n\n\n  1   2 \n347 135 \n\ntable(dat$partner_any)\n\n\n  0   1 \n203 279 \n\ndat &lt;- dat |&gt;\n  mutate(sex = factor(sex, levels = c(1, 2), labels = c(\"female\", \"male\"))) |&gt;\n  rename(partner = partner_any,\n         gender = sex,\n         friendship_importance = IMP_friends_Start)\n\n\n\n\nage_lin &lt;- lm(friendship_importance ~ age, data = dat)\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\n\n# Expand categorical predictions to plot them as step function including ribbon\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# color\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#CC79A7\"\n\n# Two extremes\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, size = .2, linetype = \"dashed\") +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\")\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_extremes.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\n# Smooth solutions\n\n# Splines\nage_splines &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\npred_splines &lt;- avg_predictions(age_splines, by = \"age\")\n\n# Polynomial\nage_poly &lt;- lm(friendship_importance ~ I(age^4) + I(age^3) + I(age^2) + age, data = dat)\npred_poly &lt;- avg_predictions(age_poly, by = \"age\")\n\n# Bins\ncutoffs &lt;- quantile(dat$age, probs = seq(0, 1, length.out = 6), na.rm = TRUE)\n\nage_bin &lt;- lm(friendship_importance ~ I(age &lt;= 22) + \n                I(age &gt; 22 & age &lt;= 29) + \n                I(age &gt; 29 & age &lt;= 39) + \n                I(age &gt; 39 & age &lt;= 50), data = dat)\npred_bin &lt;- avg_predictions(age_bin, by = \"age\")\n\ncol_neutral_1 &lt;- \"darkgrey\"\ncol_neutral_2 &lt;- \"lightgrey\"\ncol_smooth &lt;- \"#0072B2\"\n\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_splines, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_splines.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_poly, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_poly.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_bin, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_bin, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_bin.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\ncol_splines &lt;- \"#009E73\"\n\n# Disagreement: poly and splines\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_splines)"
  },
  {
    "objectID": "examples/age.html#visualize-associations-with-age",
    "href": "examples/age.html#visualize-associations-with-age",
    "title": "Age trajectory",
    "section": "",
    "text": "age_lin &lt;- lm(friendship_importance ~ age, data = dat)\nage_cat &lt;- lm(friendship_importance ~ as.factor(age), data = dat)\n\npred_lin &lt;- avg_predictions(age_lin, by = \"age\")\npred_cat &lt;- avg_predictions(age_cat, by = \"age\")\n\n# Expand categorical predictions to plot them as step function including ribbon\npred_cat_expanded &lt;- data.frame(matrix(NA, nrow = length(rep(pred_cat$estimate, each = 100)), ncol = 4))\nnames(pred_cat_expanded) &lt;- c(\"age\", \"estimate\", \"conf.low\", \"conf.high\")\npred_cat_expanded$age &lt;- seq(from = min(pred_cat$age), to = (max(pred_cat$age) + 0.999999), length.out = nrow(pred_cat_expanded))\npred_cat_expanded$estimate &lt;- rep(pred_cat$estimate, each = 100)\npred_cat_expanded$conf.low &lt;- rep(pred_cat$conf.low, each = 100)\npred_cat_expanded$conf.high &lt;- rep(pred_cat$conf.high, each = 100)\n\n# color\ncol_cat &lt;- \"#E69F00\"\ncol_lin &lt;- \"#CC79A7\"\n\n# Two extremes\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_cat, size = .2, linetype = \"dashed\") +\n  geom_ribbon(data = pred_cat_expanded, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_cat) +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_lin) +\n  geom_ribbon(data = pred_lin, aes(x = age, ymin = conf.low, ymax = conf.high), alpha = .2, fill = col_lin) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\")\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_extremes.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\n# Smooth solutions\n\n# Splines\nage_splines &lt;- lm(friendship_importance ~ bs(age, df = 4), data = dat)\npred_splines &lt;- avg_predictions(age_splines, by = \"age\")\n\n# Polynomial\nage_poly &lt;- lm(friendship_importance ~ I(age^4) + I(age^3) + I(age^2) + age, data = dat)\npred_poly &lt;- avg_predictions(age_poly, by = \"age\")\n\n# Bins\ncutoffs &lt;- quantile(dat$age, probs = seq(0, 1, length.out = 6), na.rm = TRUE)\n\nage_bin &lt;- lm(friendship_importance ~ I(age &lt;= 22) + \n                I(age &gt; 22 & age &lt;= 29) + \n                I(age &gt; 29 & age &lt;= 39) + \n                I(age &gt; 39 & age &lt;= 50), data = dat)\npred_bin &lt;- avg_predictions(age_bin, by = \"age\")\n\ncol_neutral_1 &lt;- \"darkgrey\"\ncol_neutral_2 &lt;- \"lightgrey\"\ncol_smooth &lt;- \"#0072B2\"\n\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_splines, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_splines.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_poly, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_poly.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_bin, aes(x = age, y = estimate), color = col_smooth) +\n  geom_ribbon(data = pred_bin, aes(x = age, ymin = conf.low, ymax = conf.high), fill = col_smooth, alpha = .2)\n\n\n\n\n\n\n\nggsave(here(\"plots/age_illustration/age_bin.png\"), width = 4, height = 3)\n\nError in `ggsave()`:\n! Cannot find directory '/Users/vincent/repos/psych/plots/age_illustration'.\nℹ Please supply an existing directory or use `create.dir = TRUE`.\n\ncol_splines &lt;- \"#009E73\"\n\n# Disagreement: poly and splines\nggplot() +\n  # categorical\n  geom_point(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_1, shape = 4) +\n  geom_line(data = pred_cat, aes(x = (age + 0.5), y = estimate), color = col_neutral_2, size = .2, linetype = \"dashed\") +\n  # linear\n  geom_line(data = pred_lin, aes(x = age, y = estimate), color = col_neutral_1, size = .2) +\n  coord_cartesian(ylim = c(2.5, 5)) +\n  xlab(\"Age\") +\n  ylab(\"Outcome (95% CI)\") +\n  # smoothed\n  geom_line(data = pred_poly, aes(x = age, y = estimate), color = col_smooth) +\n  geom_line(data = pred_splines, aes(x = age, y = estimate), color = col_splines)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Models as Prediction Machines",
    "section": "",
    "text": "This website hosts a replication package and extra case studies to accompany this article:\n\n“Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities” by Julia M. Rohrer and Vincent Arel-Bundock (2025)\n\nYou can find the article on: Insert Website Address\nExamples:\n\nFriends by Chance\nRelationship Status and Friendships\nAge Trajectories\n\nDownload the full replication package here.\nThe material on this site is licensed under a Creative Commons Attribution 4.0 International License. This means that you can freely share and adapt the material, as long as you cite the article listed above."
  },
  {
    "objectID": "index.html#how-to-convert-confusing-coefficients-into-clear-quantities",
    "href": "index.html#how-to-convert-confusing-coefficients-into-clear-quantities",
    "title": "Models as Prediction Machines",
    "section": "",
    "text": "This website hosts a replication package and extra case studies to accompany this article:\n\n“Models as Prediction Machines: How to Convert Confusing Coefficients into Clear Quantities” by Julia M. Rohrer and Vincent Arel-Bundock (2025)\n\nYou can find the article on: Insert Website Address\nExamples:\n\nFriends by Chance\nRelationship Status and Friendships\nAge Trajectories\n\nDownload the full replication package here.\nThe material on this site is licensed under a Creative Commons Attribution 4.0 International License. This means that you can freely share and adapt the material, as long as you cite the article listed above."
  },
  {
    "objectID": "examples/deskmates.html",
    "href": "examples/deskmates.html",
    "title": "Friends by Chance",
    "section": "",
    "text": "Before we start, let’s execute a helper script that loads the necessary dependencies.\n\nsource(here::here(\"scripts/load.R\"))\n\n\n\nThis document contains example 3, in which we analyze the effect of being seated next to each other on friendships, using Bayesian multilevel models. Data can be downloaded from the OSF but are also included in the downloadable replication package.\nEvery day experience—and previous research—suggests that being spatially close to others can result in friendships. But does spatial proximity also lead to friendships for people who are quite different from each other? We re-analyze data from a large field experiment conducted in 3rd to 8th grade classrooms in rural Hungary previously reported in Rohrer et al. (2021). Proximity was experimentally manipulated by randomizing each classrooms’ seating chart at the beginning of the school semester; thus, students randomly ended up next to each other (deskmate = 1) or not (deskmate = 0). At the end of the semester, students listed up to five best friends from their classroom, which allows us to determine which pairs of students had formed friendships (friendship = 1). Additionally, we know students’ gender and grade point average before the experiment (GPA), which allows us to investigate whether proximity also “works” for girls seated next to boys (who are quite unlikely to befriend each other at that age) or students with discrepant levels of academic achievement.\n\n\n\n\n# Read the data\ndat &lt;- read.csv(here(\"data/deskmates.csv\"))\n\n# Arbitrary subset of 60 classes to make model-fitting less tedious\ndat &lt;- dat[dat$class_id %in% unique(dat$class_id)[1:60], ]\n\n# Keep complete cases\ndat &lt;- dat[complete.cases(dat),]\n\n\n# Label focal variables\ndat &lt;- transform(dat,\n    gender_combination = factor(girl_match, label = c(\"Boys\", \"Mixed\", \"Girls\")),\n    deskmate = factor(deskmate, label = c(\"Different desk\", \"Same desk\")))\n\n# Rename to match labels in manuscript\ndat &lt;- dat |&gt;\n  rename(GPA_average = mean_gpa,\n         GPA_difference = diff_gpa,\n         friendship = friend,\n         classroom = class_id,\n         student1 = s1,\n         student2 = s2)\n\n\n# Display the first few rows of data\nhead(dat)\n\n  any_single       deskmate sim_s1_s2 girl_s1 girl_s2 roma_s1 roma_s2 gpa_s1 gpa_s2 friendship friend_s1 friend_s2 girl_match roma_match GPA_average GPA_difference sim_s1_s2_std size size.1 gendermatch classroom school_id student1 student2 gender_combination\n1          0      Same desk 0.6500000       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.2570821   18     18           0     59379        32   731891   773914              Mixed\n2          0 Different desk 0.9566667       1       1       0       0    4.4    5.0          1         1         1          2          0         4.7            0.6     1.0919846   18     18           1     59379        32   731891   492505              Girls\n3          0 Different desk 0.6366667       1       0       0       0    4.4    4.0          0         0         0          1          0         4.2            0.4    -0.3157372   18     18           0     59379        32   731891   300996              Mixed\n4          0 Different desk 0.6233333       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.3743922   18     18           0     59379        32   731891   888954              Mixed\n5          0 Different desk 0.9833333       1       1       0       0    4.4    4.6          1         1         1          2          0         4.5            0.2     1.2092947   18     18           1     59379        32   731891   706146              Girls\n6          0 Different desk 0.6233333       1       0       0       0    4.4    5.0          0         0         0          1          0         4.7            0.6    -0.3743922   18     18           0     59379        32   731891   288809              Mixed\n\n\n\n\n\nOur model here ends up a bit more complex due to the nested structure of the data, and we use the brms package (Bürkner, 2018) which allows us to fit multilevel models in a highly flexible manner. Since we now fit a Bayesian model (relying on the default priors provided by the package), marginaleffects will return credible intervals rather than confidence intervals. Our unit of observation is pairs of students, which are nested within students and classrooms. For each pair, we know: * whether they are deskmates * their gender_combination (both boys, one girl and one boy, both girls) * their GPA_average (i.e., the average across both students) and their absolute GPA_difference (i.e., the discrepancy between both students) * whether they report a friendship at the end of the experiment or not\nHere, we fit and save the model to avoid refitting repeatedly.\n\nmod &lt;- brm(\n    friendship ~ deskmate + gender_combination + deskmate:gender_combination +\n        GPA_average + GPA_difference + GPA_average:deskmate + GPA_difference:deskmate +\n        (1 | classroom) + (1 | mm(student1, student2)),\n    family = bernoulli(link = \"logit\"),\n    data = dat,\n    seed = 12345,\n    cores = 4\n)\n\n# Save model fit\nsaveRDS(mod, file = here(\"fits/deskmates.rds\"))\n\n\n# Load the fitted model\nmod &lt;- readRDS(here(\"fits/deskmates.rds\"))\n\n# Look at the model coefficients\nparameters(mod)\n\n# Fixed Effects\n\nParameter                                |   Median |         95% CI |     pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)                              |    -1.62 | [-2.34, -0.94] |   100% | 1.000 | 1791.00\ndeskmateSamedesk                         |     0.22 | [-1.49,  1.96] | 60.02% | 1.001 | 2402.00\ngender_combinationMixed                  |    -3.54 | [-3.88, -3.23] |   100% | 1.002 | 2582.00\ngender_combinationGirls                  | 5.66e-03 | [-0.25,  0.25] | 51.75% | 1.001 | 2254.00\nGPA_average                              |     0.30 | [ 0.13,  0.49] | 99.95% | 1.001 | 1495.00\nGPA_difference                           |    -0.42 | [-0.56, -0.28] |   100% | 1.000 | 4267.00\ndeskmateSamedesk:gender_combinationMixed |     0.12 | [-0.86,  0.98] | 59.62% | 1.000 | 4483.00\ndeskmateSamedesk:gender_combinationGirls |     0.04 | [-0.69,  0.78] | 54.07% | 1.001 | 4559.00\ndeskmateSamedesk:GPA_average             |     0.18 | [-0.26,  0.62] | 79.67% | 1.000 | 2529.00\ndeskmateSamedesk:GPA_difference          |    -0.03 | [-0.60,  0.47] | 54.55% | 1.000 | 4436.00\n\n\n\n\n\n\n\nIn principle, we could evaluate this model on the log-odds scale on which the coefficients are estimated.\n\navg_comparisons(mod, variables = \"deskmate\", type = \"link\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.958\n                  0.533\n                  1.35\n                \n        \n      \n    \n\n\n\nHowever, log-odds of friendship are not a particularly intuitive unit, and so instead we may want to switch to the scale of the outcome (friendships)—which is the default behavior of marginaleffects:\n\n\n\n\n# Predicted probabilities\navg_predictions(mod, by = \"deskmate\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                deskmate\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Different desk\n                  0.166\n                  0.158\n                  0.174\n                \n                \n                  Same desk\n                  0.270\n                  0.234\n                  0.311\n                \n        \n      \n    \n\n\n# Average Treatment Effect\nate &lt;- avg_comparisons(mod, variables = \"deskmate\")\nate\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.108\n                  0.0697\n                  0.15\n                \n        \n      \n    \n\n\n\nThis returns the average effect of the intervention in percentage points.\n\n\n\nOne may want to compare the effect of sitting next to each other to other interventions meant to foster friendships. For example, one staple of psychological research is the fast friends procedure in which two participants are paired up and then take turns answering questions that escalate in the degree of self-disclosure involved, from mild (“Would you like to be famous? In what way?”) to severe (“When did you last cry in front of another person?”). Echols and Ivanich (2021) implemented such a procedure in US middle school students and found that those who underwent the intervention in three sessions over three months were 10 percentage points more likely to become friends. This seems very close to the 11 percentage point effect we observed in our analysis.\nWould it be justified to conclude that the effects are practically the same?\nIn a Frequentist framework, this would be a use case for an equivalence test. Given our Bayesian model, we instead resort to the notion of a region of practical equivalence (ROPE; Kruschke, 2018; Makowski et al., 2019. First, we need to define a range around the 10 percentage points of the fast friends procedure for which we would consider the effects equivalent for practical purposes. Here, we decide that the effect ± a quarter of the effect is a sensible range, resulting in a ROPE of [0.075; 0.125].\nNow, we can calculate how likely it is that the effect of sitting next to each other falls into the ROPE of the fast friends procedure. For this, we additionally make use of the convenience provided by the posterior package:\n\nlibrary(posterior)\ndraws &lt;- get_draws(avg_comparisons(mod, variables = \"deskmate\"), \"rvar\")\n\nPr(draws$rvar &lt; 0.125) # Probability that below the upper bound\n\n[1] 0.79775\n\nPr(draws$rvar &lt; 0.075) # Probability that below the lower bound\n\n[1] 0.04175\n\n# In combination, from this we can conclude the probability that\n# the parameter lies within the bounds\nPr(draws$rvar &lt; 0.125) - Pr(draws$rvar &lt; 0.075)\n\n[1] 0.756\n\n\n\n\n\nHaving looked at the average effect of the intervention, we still do not yet know whether being seated next to each other also “works” for dissimilar students. Here, we will keep evaluating effects on the outcome scale, which we consider most intuitive. First, we can separately calculate average effects depending on gender_combination:\n\navg_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender_combination\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Boys\n                  0.1683\n                  0.07489\n                  0.2675\n                \n                \n                  Mixed\n                  0.0317\n                  0.00534\n                  0.0684\n                \n                \n                  Girls\n                  0.1940\n                  0.08685\n                  0.3018\n                \n        \n      \n    \n\n\n\n\n\nWe can easily compare all three average effects against each other in a pairwise manner:\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = ~pairwise)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (Mixed) - (Boys)\n                  -0.1355\n                  -0.2384\n                  -0.0346\n                \n                \n                  (Girls) - (Boys)\n                  0.0273\n                  -0.1169\n                  0.1704\n                \n                \n                  (Girls) - (Mixed)\n                  0.1607\n                  0.0441\n                  0.2760\n                \n        \n      \n    \n\n\n\n\n\n\nWe may also want to compare gender-matched dyads (pairs of girls, pairs of boys) with gender-mismatched dyads (pairs of a girl and a boy), (b_Two girls + b_Two boys)/2 = b_One girl one boy. This can be achieved by using a different hypothesis argument (being mindful of the order in which the groups are listed in our output):\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = \"(b1 + b3)/2 = b2\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (b1+b3)/2=b2\n                  0.149\n                  0.0667\n                  0.229\n                \n        \n      \n    \n\n\n\n\n\n\nWe can also visualize these different effects:\n\n# Plot the predicted probabilities\nplot_predictions(mod, by = c(\"gender_combination\", \"deskmate\"))\n\n\n\n\n\n\n\n# Plot the effects of the intervention in percentage points\nplot_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that so far, we have conducted counterfactual comparisons across deskmate and then simply averaged by gender_combination. Now, recall that our model also contains GPA_difference. The way we currently analyze the data, we do not condition on GPA_difference during post-estimation—that is, we do not hold it constant at any particular value. The different gender_combinations may systematically vary in their GPA_difference, which may in turn modify the effect of the intervention; these differences will show up in our average comparison by gender_combination. One could say we “marginalize over” the observed GPA_difference distribution. T his “default” behavior is different from how regression coefficients behave, which are always conditional on all other variables in the model.\nIt turns out that from a causal inference perspective (Rohrer, 2018; Wysocki et al., 2022), marginalizing over GPA_differences rather than conditioning on them is the more reasonable choice. That is because differences in GPA are plausibly causally downstream of gender differences. For gender-mismatched dyads, we may observe larger differences in GPA because gender affects GPA. Thus, if the effects of proximity are larger in gender-mismatched dyads in part because among those dyads, GPA varies more, this indirect effect should count toward the total effect modification by gender combination.\nIn contrast, when we want to evaluate how GPA_differences affect the effects of proximity, we do want to condition on gender_combination, since the variable is a plausible confounder between GPA_differences and the effects of interest. If the intervention works less for pairs with a larger GPA difference, that may simply be because those pairs are more often mismatched on gender and thus unlikely to befriend each other even when seated next to each other (for a more extensive discussion of the causal status of interactions, see Rohrer & Arslan, 2021).\n\n\nSo, does the effect of being seated next to each other (deskmate) vary by how strong their academic achievement diverges (GPA_difference), controlling for the effects of gender match (gender_combination)? We can answer this question by conducting comparisons on new hypothetical datasets in which we set GPA_difference to particular values (e.g., ± 1 SD) but keep the other variables (including gender_match) as is.\n\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                GPA_difference\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.139\n                  0.1186\n                  0.0597\n                  0.182\n                \n                \n                  1.504\n                  0.0973\n                  0.0375\n                  0.157\n                \n        \n      \n    \n\n\n# the newdata argument is crucial here: we apply the model to new data which represents a counterfactual constrast\n# between a world in which everybody's GPA difference is set to 1 SD below the mean\n# and a world in which everybody's GPA difference is set to 1 SD above the mean.\n\n# Compare the two estimates\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"),\n                hypothesis = \"b1 = b2\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b1=b2\n                  0.0212\n                  -0.0664\n                  0.116"
  },
  {
    "objectID": "examples/deskmates.html#overview",
    "href": "examples/deskmates.html#overview",
    "title": "Friends by Chance",
    "section": "",
    "text": "This document contains example 3, in which we analyze the effect of being seated next to each other on friendships, using Bayesian multilevel models. Data can be downloaded from the OSF but are also included in the downloadable replication package.\nEvery day experience—and previous research—suggests that being spatially close to others can result in friendships. But does spatial proximity also lead to friendships for people who are quite different from each other? We re-analyze data from a large field experiment conducted in 3rd to 8th grade classrooms in rural Hungary previously reported in Rohrer et al. (2021). Proximity was experimentally manipulated by randomizing each classrooms’ seating chart at the beginning of the school semester; thus, students randomly ended up next to each other (deskmate = 1) or not (deskmate = 0). At the end of the semester, students listed up to five best friends from their classroom, which allows us to determine which pairs of students had formed friendships (friendship = 1). Additionally, we know students’ gender and grade point average before the experiment (GPA), which allows us to investigate whether proximity also “works” for girls seated next to boys (who are quite unlikely to befriend each other at that age) or students with discrepant levels of academic achievement."
  },
  {
    "objectID": "examples/deskmates.html#read-and-clean-the-data",
    "href": "examples/deskmates.html#read-and-clean-the-data",
    "title": "Friends by Chance",
    "section": "",
    "text": "# Read the data\ndat &lt;- read.csv(here(\"data/deskmates.csv\"))\n\n# Arbitrary subset of 60 classes to make model-fitting less tedious\ndat &lt;- dat[dat$class_id %in% unique(dat$class_id)[1:60], ]\n\n# Keep complete cases\ndat &lt;- dat[complete.cases(dat),]\n\n\n# Label focal variables\ndat &lt;- transform(dat,\n    gender_combination = factor(girl_match, label = c(\"Boys\", \"Mixed\", \"Girls\")),\n    deskmate = factor(deskmate, label = c(\"Different desk\", \"Same desk\")))\n\n# Rename to match labels in manuscript\ndat &lt;- dat |&gt;\n  rename(GPA_average = mean_gpa,\n         GPA_difference = diff_gpa,\n         friendship = friend,\n         classroom = class_id,\n         student1 = s1,\n         student2 = s2)\n\n\n# Display the first few rows of data\nhead(dat)\n\n  any_single       deskmate sim_s1_s2 girl_s1 girl_s2 roma_s1 roma_s2 gpa_s1 gpa_s2 friendship friend_s1 friend_s2 girl_match roma_match GPA_average GPA_difference sim_s1_s2_std size size.1 gendermatch classroom school_id student1 student2 gender_combination\n1          0      Same desk 0.6500000       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.2570821   18     18           0     59379        32   731891   773914              Mixed\n2          0 Different desk 0.9566667       1       1       0       0    4.4    5.0          1         1         1          2          0         4.7            0.6     1.0919846   18     18           1     59379        32   731891   492505              Girls\n3          0 Different desk 0.6366667       1       0       0       0    4.4    4.0          0         0         0          1          0         4.2            0.4    -0.3157372   18     18           0     59379        32   731891   300996              Mixed\n4          0 Different desk 0.6233333       1       0       0       0    4.4    4.2          0         0         0          1          0         4.3            0.2    -0.3743922   18     18           0     59379        32   731891   888954              Mixed\n5          0 Different desk 0.9833333       1       1       0       0    4.4    4.6          1         1         1          2          0         4.5            0.2     1.2092947   18     18           1     59379        32   731891   706146              Girls\n6          0 Different desk 0.6233333       1       0       0       0    4.4    5.0          0         0         0          1          0         4.7            0.6    -0.3743922   18     18           0     59379        32   731891   288809              Mixed"
  },
  {
    "objectID": "examples/deskmates.html#bayesian-multilevel-regression-model",
    "href": "examples/deskmates.html#bayesian-multilevel-regression-model",
    "title": "Friends by Chance",
    "section": "",
    "text": "Our model here ends up a bit more complex due to the nested structure of the data, and we use the brms package (Bürkner, 2018) which allows us to fit multilevel models in a highly flexible manner. Since we now fit a Bayesian model (relying on the default priors provided by the package), marginaleffects will return credible intervals rather than confidence intervals. Our unit of observation is pairs of students, which are nested within students and classrooms. For each pair, we know: * whether they are deskmates * their gender_combination (both boys, one girl and one boy, both girls) * their GPA_average (i.e., the average across both students) and their absolute GPA_difference (i.e., the discrepancy between both students) * whether they report a friendship at the end of the experiment or not\nHere, we fit and save the model to avoid refitting repeatedly.\n\nmod &lt;- brm(\n    friendship ~ deskmate + gender_combination + deskmate:gender_combination +\n        GPA_average + GPA_difference + GPA_average:deskmate + GPA_difference:deskmate +\n        (1 | classroom) + (1 | mm(student1, student2)),\n    family = bernoulli(link = \"logit\"),\n    data = dat,\n    seed = 12345,\n    cores = 4\n)\n\n# Save model fit\nsaveRDS(mod, file = here(\"fits/deskmates.rds\"))\n\n\n# Load the fitted model\nmod &lt;- readRDS(here(\"fits/deskmates.rds\"))\n\n# Look at the model coefficients\nparameters(mod)\n\n# Fixed Effects\n\nParameter                                |   Median |         95% CI |     pd |  Rhat |     ESS\n-----------------------------------------------------------------------------------------------\n(Intercept)                              |    -1.62 | [-2.34, -0.94] |   100% | 1.000 | 1791.00\ndeskmateSamedesk                         |     0.22 | [-1.49,  1.96] | 60.02% | 1.001 | 2402.00\ngender_combinationMixed                  |    -3.54 | [-3.88, -3.23] |   100% | 1.002 | 2582.00\ngender_combinationGirls                  | 5.66e-03 | [-0.25,  0.25] | 51.75% | 1.001 | 2254.00\nGPA_average                              |     0.30 | [ 0.13,  0.49] | 99.95% | 1.001 | 1495.00\nGPA_difference                           |    -0.42 | [-0.56, -0.28] |   100% | 1.000 | 4267.00\ndeskmateSamedesk:gender_combinationMixed |     0.12 | [-0.86,  0.98] | 59.62% | 1.000 | 4483.00\ndeskmateSamedesk:gender_combinationGirls |     0.04 | [-0.69,  0.78] | 54.07% | 1.001 | 4559.00\ndeskmateSamedesk:GPA_average             |     0.18 | [-0.26,  0.62] | 79.67% | 1.000 | 2529.00\ndeskmateSamedesk:GPA_difference          |    -0.03 | [-0.60,  0.47] | 54.55% | 1.000 | 4436.00"
  },
  {
    "objectID": "examples/deskmates.html#interpretation-with-marginaleffects",
    "href": "examples/deskmates.html#interpretation-with-marginaleffects",
    "title": "Friends by Chance",
    "section": "",
    "text": "In principle, we could evaluate this model on the log-odds scale on which the coefficients are estimated.\n\navg_comparisons(mod, variables = \"deskmate\", type = \"link\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.958\n                  0.533\n                  1.35\n                \n        \n      \n    \n\n\n\nHowever, log-odds of friendship are not a particularly intuitive unit, and so instead we may want to switch to the scale of the outcome (friendships)—which is the default behavior of marginaleffects:\n\n\n\n\n# Predicted probabilities\navg_predictions(mod, by = \"deskmate\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                deskmate\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Different desk\n                  0.166\n                  0.158\n                  0.174\n                \n                \n                  Same desk\n                  0.270\n                  0.234\n                  0.311\n                \n        \n      \n    \n\n\n# Average Treatment Effect\nate &lt;- avg_comparisons(mod, variables = \"deskmate\")\nate\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.108\n                  0.0697\n                  0.15\n                \n        \n      \n    \n\n\n\nThis returns the average effect of the intervention in percentage points.\n\n\n\nOne may want to compare the effect of sitting next to each other to other interventions meant to foster friendships. For example, one staple of psychological research is the fast friends procedure in which two participants are paired up and then take turns answering questions that escalate in the degree of self-disclosure involved, from mild (“Would you like to be famous? In what way?”) to severe (“When did you last cry in front of another person?”). Echols and Ivanich (2021) implemented such a procedure in US middle school students and found that those who underwent the intervention in three sessions over three months were 10 percentage points more likely to become friends. This seems very close to the 11 percentage point effect we observed in our analysis.\nWould it be justified to conclude that the effects are practically the same?\nIn a Frequentist framework, this would be a use case for an equivalence test. Given our Bayesian model, we instead resort to the notion of a region of practical equivalence (ROPE; Kruschke, 2018; Makowski et al., 2019. First, we need to define a range around the 10 percentage points of the fast friends procedure for which we would consider the effects equivalent for practical purposes. Here, we decide that the effect ± a quarter of the effect is a sensible range, resulting in a ROPE of [0.075; 0.125].\nNow, we can calculate how likely it is that the effect of sitting next to each other falls into the ROPE of the fast friends procedure. For this, we additionally make use of the convenience provided by the posterior package:\n\nlibrary(posterior)\ndraws &lt;- get_draws(avg_comparisons(mod, variables = \"deskmate\"), \"rvar\")\n\nPr(draws$rvar &lt; 0.125) # Probability that below the upper bound\n\n[1] 0.79775\n\nPr(draws$rvar &lt; 0.075) # Probability that below the lower bound\n\n[1] 0.04175\n\n# In combination, from this we can conclude the probability that\n# the parameter lies within the bounds\nPr(draws$rvar &lt; 0.125) - Pr(draws$rvar &lt; 0.075)\n\n[1] 0.756\n\n\n\n\n\nHaving looked at the average effect of the intervention, we still do not yet know whether being seated next to each other also “works” for dissimilar students. Here, we will keep evaluating effects on the outcome scale, which we consider most intuitive. First, we can separately calculate average effects depending on gender_combination:\n\navg_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                gender_combination\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  Boys\n                  0.1683\n                  0.07489\n                  0.2675\n                \n                \n                  Mixed\n                  0.0317\n                  0.00534\n                  0.0684\n                \n                \n                  Girls\n                  0.1940\n                  0.08685\n                  0.3018\n                \n        \n      \n    \n\n\n\n\n\nWe can easily compare all three average effects against each other in a pairwise manner:\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = ~pairwise)\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (Mixed) - (Boys)\n                  -0.1355\n                  -0.2384\n                  -0.0346\n                \n                \n                  (Girls) - (Boys)\n                  0.0273\n                  -0.1169\n                  0.1704\n                \n                \n                  (Girls) - (Mixed)\n                  0.1607\n                  0.0441\n                  0.2760\n                \n        \n      \n    \n\n\n\n\n\n\nWe may also want to compare gender-matched dyads (pairs of girls, pairs of boys) with gender-mismatched dyads (pairs of a girl and a boy), (b_Two girls + b_Two boys)/2 = b_One girl one boy. This can be achieved by using a different hypothesis argument (being mindful of the order in which the groups are listed in our output):\n\navg_comparisons(mod,\n                variables = \"deskmate\",\n                by = \"gender_combination\",\n                hypothesis = \"(b1 + b3)/2 = b2\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  (b1+b3)/2=b2\n                  0.149\n                  0.0667\n                  0.229\n                \n        \n      \n    \n\n\n\n\n\n\nWe can also visualize these different effects:\n\n# Plot the predicted probabilities\nplot_predictions(mod, by = c(\"gender_combination\", \"deskmate\"))\n\n\n\n\n\n\n\n# Plot the effects of the intervention in percentage points\nplot_comparisons(mod, variables = \"deskmate\", by = \"gender_combination\")\n\n\n\n\n\n\n\n\n\n\n\n\nNotice that so far, we have conducted counterfactual comparisons across deskmate and then simply averaged by gender_combination. Now, recall that our model also contains GPA_difference. The way we currently analyze the data, we do not condition on GPA_difference during post-estimation—that is, we do not hold it constant at any particular value. The different gender_combinations may systematically vary in their GPA_difference, which may in turn modify the effect of the intervention; these differences will show up in our average comparison by gender_combination. One could say we “marginalize over” the observed GPA_difference distribution. T his “default” behavior is different from how regression coefficients behave, which are always conditional on all other variables in the model.\nIt turns out that from a causal inference perspective (Rohrer, 2018; Wysocki et al., 2022), marginalizing over GPA_differences rather than conditioning on them is the more reasonable choice. That is because differences in GPA are plausibly causally downstream of gender differences. For gender-mismatched dyads, we may observe larger differences in GPA because gender affects GPA. Thus, if the effects of proximity are larger in gender-mismatched dyads in part because among those dyads, GPA varies more, this indirect effect should count toward the total effect modification by gender combination.\nIn contrast, when we want to evaluate how GPA_differences affect the effects of proximity, we do want to condition on gender_combination, since the variable is a plausible confounder between GPA_differences and the effects of interest. If the intervention works less for pairs with a larger GPA difference, that may simply be because those pairs are more often mismatched on gender and thus unlikely to befriend each other even when seated next to each other (for a more extensive discussion of the causal status of interactions, see Rohrer & Arslan, 2021).\n\n\nSo, does the effect of being seated next to each other (deskmate) vary by how strong their academic achievement diverges (GPA_difference), controlling for the effects of gender match (gender_combination)? We can answer this question by conducting comparisons on new hypothetical datasets in which we set GPA_difference to particular values (e.g., ± 1 SD) but keep the other variables (including gender_match) as is.\n\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"))\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                GPA_difference\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  0.139\n                  0.1186\n                  0.0597\n                  0.182\n                \n                \n                  1.504\n                  0.0973\n                  0.0375\n                  0.157\n                \n        \n      \n    \n\n\n# the newdata argument is crucial here: we apply the model to new data which represents a counterfactual constrast\n# between a world in which everybody's GPA difference is set to 1 SD below the mean\n# and a world in which everybody's GPA difference is set to 1 SD above the mean.\n\n# Compare the two estimates\navg_comparisons(mod,\n                variables = \"deskmate\", # cause of interest\n                by = \"GPA_difference\", # variable to split by\n                newdata = datagrid(GPA_difference = c(mean(dat$GPA_difference, na.rm = TRUE) - sd(dat$GPA_difference, na.rm = TRUE), \n                                                mean(dat$GPA_difference, na.rm = TRUE) + sd(dat$GPA_difference, na.rm = TRUE)),\n                                   grid_type = \"counterfactual\"),\n                hypothesis = \"b1 = b2\")\n\n\n\n    \n\n    \n    \n      \n        \n        \n              \n                Hypothesis\n                Estimate\n                2.5 %\n                97.5 %\n              \n        \n        \n        \n                \n                  b1=b2\n                  0.0212\n                  -0.0664\n                  0.116"
  }
]